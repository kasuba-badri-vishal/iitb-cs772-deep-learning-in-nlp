{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26925dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12873193012529033824\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 14848180546737486547\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]Metal device set to: Apple M1 Pro\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 23:10:27.918681: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-15 23:10:27.918907: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7834b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Activation Function for ReLU\n",
    "def ReLU(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def derv_ReLU(z):\n",
    "    res = (z > 0).astype(float)\n",
    "    return res\n",
    "\n",
    "# Define Activation Function for Sigmoid\n",
    "def Sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "#Derivative of Sigmoid\n",
    "def derv_Sigmoid(z):\n",
    "    return Sigmoid(z)*(1-Sigmoid(z))\n",
    "\n",
    "# Early Stopping criteria\n",
    "def early_stopping(old_err, curr_err):\n",
    "    if(old_err < curr_err):\n",
    "        print(\"Early Stopping\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Batch Normalization\n",
    "def batch_normalize(x):\n",
    "    mean_vec = x.mean(axis=0)\n",
    "    std_vec = x.std(axis=0)\n",
    "    mean_vec = mean_vec.reshape(1,mean_vec.shape[0])\n",
    "    std_vec = std_vec.reshape(1,std_vec.shape[0])\n",
    "    return (x - mean_vec)/std_vec\n",
    "\n",
    "def min_max_scaleX01(data): \n",
    "    return (data - data.min(axis=0))/(data.max(axis=0) - data.min(axis=0))\n",
    "\n",
    "def min_max_scaleY01(y):    \n",
    "    return (y-1922)/(2010-1922) \n",
    "\n",
    "def min_max_scaleX11(data): \n",
    "    return 2*((data - data.min(axis=0))/(data.max(axis=0) - data.min(axis=0))) - 1\n",
    "\n",
    "def min_max_scaleY11(y):    \n",
    "    return 2*((y-1922)/(2010-1922)) - 1 \n",
    "\n",
    "def min_max_scaling(train_input, train_target, dev_input, dev_target, test_input):\n",
    "    train_input = min_max_scaleX01(train_input)\n",
    "    train_target = min_max_scaleY01(train_target)\n",
    "    dev_input = min_max_scaleX01(dev_input)\n",
    "    dev_target = min_max_scaleY01(dev_target)\n",
    "    test_input = min_max_scaleX01(test_input)\n",
    "    return train_input, train_target, dev_input, dev_target, test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_layers, num_units, selected_features):\n",
    "        self.num_layers = num_layers\n",
    "        self.num_units = num_units\n",
    "\n",
    "        self.biases = []\n",
    "        self.weights = []\n",
    "        for i in range(num_layers):\n",
    "\n",
    "            if i==0:\n",
    "                # Input layer\n",
    "                self.weights.append(np.random.uniform(-1, 1, size=(selected_features, self.num_units)))\n",
    "            else:\n",
    "                # Hidden layer\n",
    "                self.weights.append(np.random.uniform(-1, 1, size=(self.num_units, self.num_units)))\n",
    "\n",
    "            self.biases.append(np.random.uniform(-1, 1, size=(self.num_units, 1)))\n",
    "\n",
    "        # Output layer\n",
    "        self.biases.append(np.random.uniform(-1, 1, size=(1, 1)))\n",
    "        self.weights.append(np.random.uniform(-1, 1, size=(self.num_units, selected_features)))\n",
    "\n",
    "    def __call__(self, X):\n",
    "        self.Z=[]\n",
    "        self.A=[]   \n",
    "        \n",
    "        z_temp = X@self.weights[0] + (self.biases[0]).T\n",
    "        z_temp = batch_normalize(z_temp)\n",
    "    \n",
    "        self.Z.append(z_temp)\n",
    "        self.A.append(ReLU(self.Z[0]))\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            z_temp = self.A[i-1]@self.weights[i] + (self.biases[i]).T\n",
    "            z_temp = batch_normalize(z_temp)\n",
    "            self.Z.append(z_temp)\n",
    "            self.A.append(ReLU(self.Z[i]))\n",
    "        \n",
    "           \n",
    "        z_temp = self.A[self.num_layers-1]@self.weights[self.num_layers] + (self.biases[self.num_layers]).T\n",
    "        # z_temp = batch_normalize(z_temp)\n",
    "        # self.Z.append(z_temp)\n",
    "        exp_scores = np.exp(z)\n",
    "        softmax_scores = exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n",
    "        self.Z.append(softmax_scores)\n",
    "        \n",
    "        self.y_pred = self.Z[self.num_layers]\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, X, y, lamda):\n",
    "\n",
    "        dz = []\n",
    "        dA = []\n",
    "        del_W = []\n",
    "        del_b = []\n",
    "        m = len(y)\n",
    "        \n",
    "        dz.insert(0, self.y_pred - y)\n",
    "        del_W.insert(0, np.matmul(self.A[self.num_layers-1].T,dz[0]) + lamda*self.weights[self.num_layers])\n",
    "        del_b.insert(0, np.matmul(np.ones((m,1)).T,dz[0]) + lamda*self.biases[self.num_layers])\n",
    "        dA.insert(0, np.matmul(dz[0],self.weights[self.num_layers].T))*(1 - np.power(self.A[self.num_layers-1], 2))\n",
    "\n",
    "\n",
    "        # Wt optimization for wlast yet to be written\n",
    "        for i in range(self.num_layers-1, 0, -1) :\n",
    "\n",
    "            dz.insert(0, np.multiply(dA[0],derv_ReLU(self.Z[i])))\n",
    "            del_W.insert(0, np.matmul(self.A[i-1].T,dz[0]) + lamda*self.weights[i])\n",
    "            \n",
    "            db_cl = dz[0].sum(axis=0)\n",
    "            del_b.insert(0, db_cl.reshape(db_cl.shape[0],1) + lamda*self.biases[i])\n",
    "            dA.insert(0, np.matmul(dz[0],self.weights[i].T))\n",
    "\n",
    "\n",
    "        dz.insert(0, np.multiply(dA[0], derv_ReLU(self.Z[0])))\n",
    "        db_cl = dz[0].sum(axis=0)\n",
    "        del_b.insert(0, db_cl.reshape(db_cl.shape[0],1) + lamda*self.biases[0])\n",
    "        del_W.insert(0, np.matmul(X.T, dz[0]) + lamda*self.weights[0])\n",
    "\n",
    "        return del_W, del_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fbf0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    def __init__(self, learning_rate, optimizer):\n",
    "        '''\n",
    "        Create a Gradient Descent based optimizer with given\n",
    "        learning rate.\n",
    "        Other parameters can also be passed to create different types of\n",
    "        optimizers.\n",
    "        Hint: You can use the class members to track various states of the\n",
    "        optimizer.\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.momentum_beta = 0.9\n",
    "        self.theta_weights = []\n",
    "        self.theta_biases = []\n",
    "        \n",
    "        \n",
    "        self.learning_rate_weights = learning_rate\n",
    "        self.learning_rate_biases = learning_rate\n",
    "        \n",
    "        self.rmsprop_beta = 0.8\n",
    "        self.weights_gamma = 0\n",
    "        self.biases_gamma = 0\n",
    "        self.weights_epsilon = 0.9\n",
    "        self.biases_epsilon = 0.9\n",
    "\n",
    "    def step(self, weights, biases, delta_weights, delta_biases):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "            weights: Current weights of the network.\n",
    "            biases: Current biases of the network.\n",
    "            delta_weights: Gradients of weights with respect to loss.\n",
    "            delta_biases: Gradients of biases with respect to loss.\n",
    "        '''\n",
    "        if(self.optimizer == \"gradient\"):\n",
    "            return self.gradient(weights, biases, delta_weights, delta_biases)\n",
    "        elif(self.optimizer == \"adam\"):\n",
    "            return self.adam(weights, biases, delta_weights, delta_biases)\n",
    "        elif(self.optimizer == \"rmsprop\"):\n",
    "            return self.rmsprop(weights, biases, delta_weights, delta_biases)\n",
    "        elif(self.optimizer == \"momentum\"):\n",
    "            return self.momentum(weights, biases, delta_weights, delta_biases)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def gradient(self, weights, biases, delta_weights, delta_biases):\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i] - self.learning_rate_weights*delta_weights[i]\n",
    "            biases[i] = biases[i] - self.learning_rate_biases*delta_biases[i]\n",
    "        return weights, biases\n",
    "    \n",
    "    def momentum(self, weights, biases, delta_weights, delta_biases):\n",
    "        '''\n",
    "        Momentum Optimizer\n",
    "        '''\n",
    "        if(len(self.theta_weights)==0):\n",
    "            self.get_theta_params(delta_weights, delta_biases)\n",
    "        \n",
    "        self.update_theta_params(delta_weights, delta_biases)\n",
    "        \n",
    "        return self.gradient(weights, biases, self.theta_weights, self.theta_biases)\n",
    "       \n",
    "    \n",
    "    def rmsprop(self, weights, biases, delta_weights, delta_biases):\n",
    "        '''\n",
    "        RMSProp Optimizer\n",
    "        '''       \n",
    "        self.get_gamma_params(delta_weights, delta_biases)\n",
    "        \n",
    "        return self.gradient(weights, biases, delta_weights, delta_biases)\n",
    "    \n",
    "    def adam(self, weights, biases, delta_weights, delta_biases):\n",
    "        '''\n",
    "        ADAM Optimizer\n",
    "        '''       \n",
    "        if(len(self.theta_weights)==0):\n",
    "            self.get_theta_params(delta_weights, delta_biases)\n",
    "            \n",
    "        self.update_theta_params(delta_weights, delta_biases)\n",
    "        \n",
    "        self.get_gamma_params(delta_weights, delta_biases)\n",
    "        \n",
    "        return self.gradient(weights, biases, self.theta_weights, self.theta_biases)\n",
    "        \n",
    "    \n",
    "    def get_gamma_params(self, delta_weights, delta_biases):\n",
    "        \n",
    "        delta_weights_square = 0\n",
    "        for i in delta_weights:\n",
    "            delta_weights_square += np.sum(i**2)\n",
    "            \n",
    "        delta_biases_square = 0\n",
    "        for i in delta_biases:\n",
    "            delta_biases_square += np.sum(i**2)\n",
    "            \n",
    "        \n",
    "        self.weights_gamma = self.rmsprop_beta*self.weights_gamma + (1-self.rmsprop_beta)*delta_weights_square\n",
    "        self.biases_gamma = self.rmsprop_beta*self.biases_gamma + (1-self.rmsprop_beta)*delta_biases_square\n",
    "        \n",
    "        self.learning_rate_weights = self.learning_rate_weights/math.sqrt(self.weights_gamma + self.weights_epsilon)\n",
    "        self.learning_rate_biases = self.learning_rate_biases/math.sqrt(self.biases_gamma + self.biases_epsilon)\n",
    "        \n",
    "    \n",
    "    def get_theta_params(self, weights, biases):\n",
    "        x = np.copy(weights)\n",
    "        for i in x:\n",
    "            i.fill(0)\n",
    "            self.theta_weights.append(i)\n",
    "            \n",
    "        x = np.copy(biases)\n",
    "        for i in x:\n",
    "            i.fill(0)\n",
    "            self.theta_biases.append(i)\n",
    "            \n",
    "    def update_theta_params(self, delta_weights, delta_biases):\n",
    "        for i in range(len(self.theta_weights)):\n",
    "            self.theta_weights[i] = self.momentum_beta*self.theta_weights[i] + (1-self.momentum_beta)*delta_weights[i]\n",
    "            self.theta_biases[i] = self.momentum_beta*self.theta_biases[i] + (1-self.momentum_beta)*delta_biases[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ffb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6dbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Hyper-parameters \n",
    "    max_epochs = 50\n",
    "    learning_rate = 2e-3\n",
    "    lamda = 0 # Regularization Parameter\n",
    "    batch_size = 1024\n",
    "    \n",
    "    \n",
    "    net = Net()\n",
    "    optimizer = Optimizer(learning_rate, net.weights, net.biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
