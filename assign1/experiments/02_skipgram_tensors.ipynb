{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94qTFyHNI1Li",
    "outputId": "acccb9af-63dc-43d3-beda-23aed87a8a43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "text_counter = Counter()\n",
    "tqdm.pandas()\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relu(z):\n",
    "#     return np.maximum(0.1,z)\n",
    "# def relu_prime(z):\n",
    "#     return np.where(z > 0, 1.0, 0.1)\n",
    "# def softmax(z):\n",
    "#     return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "# def sigmoid_prime(z):\n",
    "#     return z * (1 - z)\n",
    "\n",
    "# def get_similarity(word1, word2):\n",
    "#     w1_emb = get_embeddings(word1)\n",
    "#     w2_emb = get_embeddings(word2)\n",
    "#     return cosine_similarity(w1_emb, w2_emb)\n",
    "\n",
    "# def cosine_similarity(A, B):\n",
    "#     C = np.squeeze(np.asarray(A))\n",
    "#     D = np.squeeze(np.asarray(B))\n",
    "#     return np.dot(C, D) / (np.linalg.norm(C) * np.linalg.norm(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word):\n",
    "    try:\n",
    "        val = word2idx[word]\n",
    "        input_arr = torch.zeros((vocab_size), dtype=torch.float, device=mps_device)\n",
    "        input_arr[val] = 1\n",
    "        emb = torch.matmul(input_arr,net.weights[0]) + net.biases[0]\n",
    "        return emb\n",
    "    except:\n",
    "        return torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "    \n",
    "def get_accuracy(validation):\n",
    "    accuracy = torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "    for _, row in validation.iterrows():\n",
    "        w1_emb = get_embeddings(row['word1'])\n",
    "        w2_emb = get_embeddings(row['word2'])    \n",
    "        w3_emb = get_embeddings(row['word3'])\n",
    "        w4_emb = get_embeddings(row['word4'])\n",
    "        \n",
    "        if((torch.numel(w1_emb) != 0) and (torch.numel(w2_emb) != 0) and (torch.numel(w3_emb) != 0)):\n",
    "            pred_emb = w1_emb + w3_emb - w2_emb\n",
    "            accuracy = torch.cat(accuracy, cosine_similarity(pred_emb, w4_emb), dim=0)\n",
    "    return torch.mean(accuracy, dim=0)\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    C = torch.squeeze(A)\n",
    "    D = torch.squeeze(B)\n",
    "    return torch.dot(C, D) / (torch.norm(C) * torch.norm(D))\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -torch.mean(torch.sum(y_true * torch.log(y_pred), axis=-1))\n",
    "\n",
    "def get_dataset(words, window_size):\n",
    "    data = pd.DataFrame(columns=[\"word\", \"context_words\"])\n",
    "    for index, word in enumerate(words):\n",
    "        context_words = words[max(0, index - window_size): index] + words[index + 1: index + window_size + 1]\n",
    "        context_words = list(set(context_words))\n",
    "        data.loc[len(data.index)] = [word, context_words]\n",
    "    global dataset\n",
    "    dataset = pd.concat([dataset,data])\n",
    "    \n",
    "    \n",
    "def get_batch_data(start_index, end_index):\n",
    "\n",
    "    batch_dataset = dataset[start_index:end_index].reset_index(drop=True)\n",
    "    batch_size = batch_dataset.shape[0]\n",
    "    batch_input = torch.zeros([batch_size, vocab_size], dtype=torch.float, device=mps_device)\n",
    "    batch_output = torch.zeros([batch_size, vocab_size], dtype=torch.float, device=mps_device)\n",
    "\n",
    "    for index, data in batch_dataset.iterrows():\n",
    "        batch_input[index, data['word']] = 1\n",
    "        for ind in data['context_words']:\n",
    "            batch_output[index, ind] = 1\n",
    "            \n",
    "    return batch_input, batch_output\n",
    "\n",
    "\n",
    "def train(net, optimizer, lamda, max_epochs, dev_input, dev_target, batch_size, train_size):\n",
    "\n",
    "\n",
    "    for e in range(max_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for start_index in range(0, train_size, batch_size):\n",
    "            end_index = min(start_index + batch_size, train_size)\n",
    "            batch_input, batch_target = get_batch_data(start_index, end_index)\n",
    "            pred = net(batch_input)\n",
    "\n",
    "            # Compute gradients of loss w.r.t. weights and biases\n",
    "            dW, db = net.backward(batch_input, batch_target, lamda)\n",
    "\n",
    "            # Get updated weights based on current weights and gradients\n",
    "            weights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)\n",
    "\n",
    "            # Update model's weights and biases\n",
    "            net.weights = weights_updated\n",
    "            net.biases = biases_updated\n",
    "            print(e, start_index, cross_entropy_loss(pred, batch_target).item())\n",
    "\n",
    "        dev_pred = net(dev_input)\n",
    "        indices = topk_indices = torch.topk(dev_pred, k=window_size, dim=1)[1][:, -window_size:]\n",
    "        converted_matrix = torch.zeros_like(dev_pred)\n",
    "        converted_matrix[torch.arange(dev_pred.shape[0])[:, None], indices] = 1\n",
    "        numpy_dev_target = dev_target.cpu().numpy()\n",
    "        converted_matrix = converted_matrix.cpu().numpy()\n",
    "        print('F1 Score on dev data: {:.5f}'.format(f1_score(numpy_dev_target, converted_matrix, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.y_pred = None\n",
    "        self.emb    = None\n",
    "        self.weights = []\n",
    "        self.biases  = []\n",
    "        \n",
    "        self.weights.append(torch.rand((vocab_size, embedding_size), device=mps_device) * 2 - 1)\n",
    "        self.weights.append(torch.rand((embedding_size, vocab_size), device=mps_device) * 2 - 1)\n",
    "        self.biases.append(torch.rand((embedding_size), device=mps_device) * 2 - 1)\n",
    "        self.biases.append(torch.rand((vocab_size), device=mps_device) * 2 - 1)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        self.emb = (torch.matmul(X,self.weights[0]) + self.biases[0])\n",
    "        self.y_pred = torch.softmax(torch.matmul(self.emb, self.weights[1]) + self.biases[1], dim=0)\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, X, y, lamda):\n",
    "        \n",
    "        del_W = []\n",
    "        del_b = []\n",
    "\n",
    "        delta = self.y_pred - y\n",
    "        del_b.insert(0,torch.sum(delta, axis = 0, keepdims = True))\n",
    "        del_W.insert(0,torch.matmul(self.emb.T, delta) + lamda * (self.weights[1]))\n",
    "\n",
    "        delta = torch.matmul(delta, self.weights[1].T) * (self.emb)\n",
    "        del_b.insert(0,torch.sum(delta, axis = 0, keepdims = True))\n",
    "        del_W.insert(0,torch.matmul(X.T, delta) + lamda * (self.weights[0]))\n",
    "        return del_W, del_b\n",
    "    \n",
    "class Optimizer(object):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, learning_rate, weights, biases, optimizer=\"gradient\"):\n",
    "        \n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.m_dw = [torch.zeros((w.shape), device=mps_device) for w in weights]\n",
    "        self.m_db = [torch.zeros((b.shape), device=mps_device) for b in biases]\n",
    "        self.v_dw = [torch.zeros((w.shape), device=mps_device) for w in weights]\n",
    "        self.v_db = [torch.zeros((b.shape), device=mps_device) for b in biases]\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.eta = learning_rate\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, weights, biases, delta_weights, delta_biases):\n",
    "        \n",
    "        if(self.optimizer == \"gradient\"):\n",
    "            return self.gradient(weights, biases, delta_weights, delta_biases)\n",
    "        elif(self.optimizer == \"adam\"):\n",
    "            return self.adam(weights, biases, delta_weights, delta_biases)\n",
    "    \n",
    "    def adam(self, weights, biases, delta_weights, delta_biases):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_dw = [self.beta1 * m + (1 - self.beta1) * del_w for m, del_w in zip(self.m_dw, delta_weights)]\n",
    "        self.m_db = [self.beta1 * m + (1 - self.beta1) * del_b for m, del_b in zip(self.m_db, delta_biases)]\n",
    "        self.v_dw = [self.beta2 * v + (1 - self.beta2) * (del_w**2) for v, del_w in zip(self.v_dw, delta_weights)]\n",
    "        self.v_db = [self.beta2 * v + (1 - self.beta2) * (del_b**2) for v, del_b in zip(self.v_db, delta_biases)]\n",
    "\n",
    "        # bias correction\n",
    "        m_hat_dw = [m / (1 - self.beta1 ** self.t) for m in self.m_dw]\n",
    "        v_hat_dw = [v / (1 - self.beta2 ** self.t) for v in self.v_dw]\n",
    "\n",
    "        m_hat_db = [m / (1 - self.beta1 ** self.t) for m in self.m_db]\n",
    "        v_hat_db = [v / (1 - self.beta2 ** self.t) for v in self.v_db]\n",
    "\n",
    "        # update weights and biases\n",
    "        weights = [w - self.eta * m_hat / ((torch.sqrt(v_hat) + self.epsilon)) for w, m_hat, v_hat in zip(weights, m_hat_dw, v_hat_dw)] \n",
    "        biases = [b - self.eta * m_hat / ((torch.sqrt(v_hat) + self.epsilon)) for b, m_hat, v_hat in zip(biases, m_hat_db, v_hat_db)]\n",
    "        return weights, biases\n",
    "\n",
    "    \n",
    "    def gradient(self, weights, biases, delta_weights, delta_biases):\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i] - self.eta*delta_weights[i]\n",
    "            biases[i] = biases[i] - self.eta*delta_biases[i]\n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DDS1h9zLCkdr"
   },
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = './../data/weights/'\n",
    "DATASET_PATH = './../data/processed/processed_data.csv'\n",
    "\n",
    "embedding_size = 512\n",
    "no_of_rows     = 50000\n",
    "window_size    = 3\n",
    "num_neg_sam    = 0\n",
    "max_epochs     = 25\n",
    "learning_rate  = 0.00001\n",
    "reg_lambda     = 1\n",
    "batch_size     = 16192\n",
    "train_split    = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:47<00:00, 219.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length : 39249\n",
      "Dataset size : 390992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH ,nrows = no_of_rows)\n",
    "df = df.dropna()\n",
    "\n",
    "df['sentences'] = df['sentences'].apply(lambda sentence : sentence.split())\n",
    "_ = df['sentences'].apply(text_counter.update)\n",
    "text_counter.update(['UNK'])\n",
    "vocab_size = len(text_counter)\n",
    "\n",
    "words, _ = zip(*text_counter.most_common(vocab_size))\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "idx2word = {i: w for i, w in enumerate(words)}\n",
    "\n",
    "\n",
    "# dataset = pd.read_pickle('dataset.pkl')\n",
    "df['sentences'] = df['sentences'].apply(lambda words : [word2idx[word] for word in words])\n",
    "\n",
    "dataset = pd.DataFrame(columns=[\"word\", \"context_words\"])\n",
    "_ = df['sentences'].progress_apply(lambda x : get_dataset(x, window_size))\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "data_size = dataset.shape[0]\n",
    "\n",
    "print(\"Vocab length :\", vocab_size)\n",
    "print(\"Dataset size :\", data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_pickle('dataset.pkl')\n",
    "\n",
    "# dataset = pd.read_pickle('dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = max(int(0.90*data_size), data_size-5000)\n",
    "\n",
    "net = Net()\n",
    "optimizer = Optimizer(learning_rate, net.weights, net.biases, optimizer=\"adam\")\n",
    "dev_input, dev_target = get_batch_data(train_size, data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 130.64962768554688\n",
      "0 16192 130.43853759765625\n",
      "0 32384 129.64976501464844\n",
      "0 48576 130.74632263183594\n",
      "0 64768 130.70191955566406\n",
      "0 80960 130.88084411621094\n",
      "0 97152 130.0386962890625\n",
      "0 113344 130.288330078125\n",
      "0 129536 130.94837951660156\n",
      "0 145728 130.67413330078125\n",
      "0 161920 130.2542266845703\n",
      "0 178112 129.99269104003906\n",
      "0 194304 130.4842529296875\n",
      "0 210496 129.7842559814453\n",
      "0 226688 129.8375244140625\n",
      "0 242880 130.16464233398438\n",
      "0 259072 130.3294219970703\n",
      "0 275264 130.7039337158203\n",
      "0 291456 130.45770263671875\n",
      "0 307648 130.22764587402344\n",
      "0 323840 131.07582092285156\n",
      "0 340032 130.126708984375\n",
      "0 356224 130.79876708984375\n",
      "0 372416 129.3010711669922\n",
      "F1 Score on dev data: 0.00011\n",
      "1 0 130.60684204101562\n",
      "1 16192 130.39596557617188\n",
      "1 32384 129.60792541503906\n",
      "1 48576 130.7040557861328\n",
      "1 64768 130.6595916748047\n",
      "1 80960 130.83865356445312\n",
      "1 97152 129.99658203125\n",
      "1 113344 130.2463836669922\n",
      "1 129536 130.90611267089844\n",
      "1 145728 130.63157653808594\n",
      "1 161920 130.21202087402344\n",
      "1 178112 129.9503173828125\n",
      "1 194304 130.44241333007812\n",
      "1 210496 129.74241638183594\n",
      "1 226688 129.79537963867188\n",
      "1 242880 130.12261962890625\n",
      "1 259072 130.2873077392578\n",
      "1 275264 130.66156005859375\n",
      "1 291456 130.41531372070312\n",
      "1 307648 130.18496704101562\n",
      "1 323840 131.033203125\n",
      "1 340032 130.08424377441406\n",
      "1 356224 130.75604248046875\n",
      "1 372416 129.2592315673828\n",
      "F1 Score on dev data: 0.00011\n",
      "2 0 130.565185546875\n",
      "2 16192 130.3540802001953\n",
      "2 32384 129.5665740966797\n",
      "2 48576 130.66221618652344\n",
      "2 64768 130.6176300048828\n",
      "2 80960 130.79678344726562\n",
      "2 97152 129.95474243164062\n",
      "2 113344 130.20465087890625\n",
      "2 129536 130.86404418945312\n",
      "2 145728 130.58920288085938\n",
      "2 161920 130.17002868652344\n",
      "2 178112 129.90811157226562\n",
      "2 194304 130.40072631835938\n",
      "2 210496 129.70074462890625\n",
      "2 226688 129.75343322753906\n",
      "2 242880 130.08071899414062\n",
      "2 259072 130.2453155517578\n",
      "2 275264 130.61932373046875\n",
      "2 291456 130.37306213378906\n",
      "2 307648 130.14248657226562\n",
      "2 323840 130.99075317382812\n",
      "2 340032 130.04197692871094\n",
      "2 356224 130.71347045898438\n",
      "2 372416 129.21754455566406\n",
      "F1 Score on dev data: 0.00011\n",
      "3 0 130.52342224121094\n",
      "3 16192 130.31210327148438\n",
      "3 32384 129.52520751953125\n",
      "3 48576 130.62034606933594\n",
      "3 64768 130.5756072998047\n",
      "3 80960 130.7548828125\n",
      "3 97152 129.91285705566406\n",
      "3 113344 130.16290283203125\n",
      "3 129536 130.82199096679688\n",
      "3 145728 130.54684448242188\n",
      "3 161920 130.12806701660156\n",
      "3 178112 129.86595153808594\n",
      "3 194304 130.35910034179688\n",
      "3 210496 129.6591339111328\n",
      "3 226688 129.71153259277344\n",
      "3 242880 130.03890991210938\n",
      "3 259072 130.2034454345703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, optimizer, lamda, max_epochs, dev_input, dev_target, batch_size, train_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m     net\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m weights_updated\n\u001b[1;32m     76\u001b[0m     net\u001b[38;5;241m.\u001b[39mbiases \u001b[38;5;241m=\u001b[39m biases_updated\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e, start_index, \u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_target\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     79\u001b[0m dev_pred \u001b[38;5;241m=\u001b[39m net(dev_input)\n\u001b[1;32m     80\u001b[0m indices \u001b[38;5;241m=\u001b[39m topk_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(dev_pred, k\u001b[38;5;241m=\u001b[39mwindow_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m][:, \u001b[38;5;241m-\u001b[39mwindow_size:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net, optimizer, reg_lambda, max_epochs, dev_input, dev_target, batch_size, train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word):\n",
    "    try:\n",
    "        val = word2idx[word]\n",
    "        input_arr = torch.zeros((vocab_size), dtype=torch.float, device=mps_device)\n",
    "        input_arr[val] = 1\n",
    "        emb = torch.matmul(input_arr,net.weights[0]) + net.biases[0]\n",
    "        return emb\n",
    "    except:\n",
    "        return torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "\n",
    "def get_result(word1, word2, word3, problem='skipgram', window=4):\n",
    "    w1_emb = get_embeddings(word1)\n",
    "    w2_emb = get_embeddings(word2)    \n",
    "    w3_emb = get_embeddings(word3)    \n",
    "    if((torch.numel(w1_emb) != 0) and (torch.numel(w2_emb) != 0) and (torch.numel(w3_emb) != 0)):\n",
    "        w4_emb = w1_emb + w3_emb - w2_emb\n",
    "        output = torch.softmax(torch.matmul(w4_emb, net.weights[1]) + net.biases[1], dim=1)\n",
    "#         print(output.shape)\n",
    "#         print(output.cpu().unique(return_counts=True))\n",
    "        if(problem=='skipgram'):\n",
    "            topk_preds = torch.topk(output, k=window).indices.tolist()\n",
    "            ans = [idx2word[x] for x in topk_preds[0]]\n",
    "        else:\n",
    "            ans = idx2word[torch.argmax(output).item()]\n",
    "        return ans\n",
    "    return \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.softmax(torch.matmul(get_embeddings(validation['word1'][idx]), net.weights[1])+net.biases[1],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word[torch.argmax(x).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(get_embeddings(validation['word1'][idx]), net.weights[1])+net.biases[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.cpu().unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 35\n",
    "get_result(validation['word1'][idx],validation['word2'][idx],validation['word3'][idx],problem='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 991/991 [00:03<00:00, 303.63it/s]\n"
     ]
    }
   ],
   "source": [
    "validation = pd.read_csv('./../data/Validation.txt', sep=' ', names=['word1','word2','word3','word4'])\n",
    "\n",
    "validation['word1'] = validation['word1'].apply(lambda x : x.lower())\n",
    "validation['word2'] = validation['word2'].apply(lambda x : x.lower())\n",
    "validation['word3'] = validation['word3'].apply(lambda x : x.lower())\n",
    "validation['word4'] = validation['word4'].apply(lambda x : x.lower())\n",
    "\n",
    "validation['result'] = validation[['word1','word2','word3']].progress_apply(lambda x : get_result(x['word1'], x['word2'], x['word3'], problem='cbow'), axis = 1)\n",
    "\n",
    "# print(\"Accuracy is :\", get_accuracy(validation))\n",
    "# print(\"Coverage is :\", validation[validation['result']!=\"\"].shape[0]/991)\n",
    "# validation[validation['result']==validation['word4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>see</td>\n",
       "      <td>sees</td>\n",
       "      <td>nagvanshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>shuffle</td>\n",
       "      <td>shuffles</td>\n",
       "      <td>silent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sing</td>\n",
       "      <td>sings</td>\n",
       "      <td>countercoup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sit</td>\n",
       "      <td>sits</td>\n",
       "      <td>souqs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>slow</td>\n",
       "      <td>slows</td>\n",
       "      <td>raise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>naira</td>\n",
       "      <td>grants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>empresarial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>japan</td>\n",
       "      <td>yen</td>\n",
       "      <td>discouraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>puddle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>denmark</td>\n",
       "      <td>krone</td>\n",
       "      <td>kristiansand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1  word2    word3     word4        result\n",
       "0         walk  walks      see      sees     nagvanshi\n",
       "1         walk  walks  shuffle  shuffles        silent\n",
       "2         walk  walks     sing     sings   countercoup\n",
       "3         walk  walks      sit      sits         souqs\n",
       "4         walk  walks     slow     slows         raise\n",
       "..         ...    ...      ...       ...           ...\n",
       "986  argentina   peso  nigeria     naira        grants\n",
       "987  argentina   peso     iran      rial   empresarial\n",
       "988  argentina   peso    japan       yen   discouraged\n",
       "989      india  rupee     iran      rial        puddle\n",
       "990      india  rupee  denmark     krone  kristiansand\n",
       "\n",
       "[991 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 991/991 [00:00<00:00, 80503.47it/s]\n"
     ]
    }
   ],
   "source": [
    "validation['found'] = validation[['word4','result']].progress_apply(lambda x : x['word4'] in x['result'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    991\n",
       "Name: found, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation['found'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[validation['result']!=\"\"].shape[0]/991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>convenient</td>\n",
       "      <td>inconvenient</td>\n",
       "      <td>clear</td>\n",
       "      <td>unclear</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1         word2  word3    word4   result\n",
       "872  convenient  inconvenient  clear  unclear  unclear"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[validation['result']==validation['word4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(WEIGHTS_PATH + 'skipgram_weights_' + str(vocab_size) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(net.weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(WEIGHTS_PATH + 'skipgram_biases_' + str(vocab_size) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(net.biases, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.weights, WEIGHTS_PATH + 'skipgram_weights_' + str(vocab_size) + '.pt')\n",
    "torch.save(net.biases, WEIGHTS_PATH + 'skipgram_biases_' + str(vocab_size) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.weights = torch.load(WEIGHTS_PATH + 'skipgram_weights_' + str(vocab_size) + '.pt')\n",
    "net.biases = torch.load(WEIGHTS_PATH + 'skipgram_biases_' + str(vocab_size) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNPkKQMSJVmw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/Validation.txt', sep=' ', names=['w1','w2','w3','w4'])\n",
    "df2 = df[['w3','w4']].rename(columns={'w3':'w1', 'w4':'w2'})\n",
    "df = pd.concat([df[['w1','w2']],df2], ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['w1'] = df['w1'].apply(lambda x : x.lower())\n",
    "df['w2'] = df['w2'].apply(lambda x : x.lower())\n",
    "\n",
    "df['w1_present'] = df['w1'].apply(lambda x : x in result)\n",
    "df['w2_present'] = df['w2'].apply(lambda x : x in result)\n",
    "\n",
    "print(df['w1_present'].value_counts())\n",
    "print(df['w2_present'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
