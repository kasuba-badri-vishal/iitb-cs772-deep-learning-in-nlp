{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94qTFyHNI1Li",
    "outputId": "acccb9af-63dc-43d3-beda-23aed87a8a43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "DDS1h9zLCkdr"
   },
   "outputs": [],
   "source": [
    "embedding_size = 8192\n",
    "nrows = 10000\n",
    "window_size = 3\n",
    "num_neg_samples = 3\n",
    "WEIGHTS_PATH = './../data/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufkj_WKJI7J_",
    "outputId": "92a4f21f-33c6-47c7-8ef2-963d9c46d682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens :  33086\n",
      "Vocabulary Length :  5171\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./../data/processed/processed_data_2_no_guttenberg.csv', nrows = nrows)\n",
    "df = df.dropna()\n",
    "\n",
    "result = ''\n",
    "for row in df['sentences']:\n",
    "    row += ' '\n",
    "    result += row\n",
    "\n",
    "# Tokenize the book\n",
    "tokens = nltk.word_tokenize(result)\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"No. of tokens : \",len(tokens))\n",
    "print(\"Vocabulary Length : \", len(vocab))\n",
    "\n",
    "# Create the word-to-index and index-to-word dictionaries\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "d6oUpjJ80NWy"
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0.1,z)\n",
    "def relu_prime(z):\n",
    "    return np.where(z > 0, 1.0, 0.1)\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    C = np.squeeze(np.asarray(A))\n",
    "    D = np.squeeze(np.asarray(B))\n",
    "    return np.dot(C, D) / (np.linalg.norm(C) * np.linalg.norm(D))\n",
    "\n",
    "def get_embeddings(word):\n",
    "    try:\n",
    "        val = word2idx[word]\n",
    "        input_arr = np.zeros((vocab_size), dtype=np.float32)\n",
    "        input_arr[val] = 1\n",
    "        emb = (input_arr @ net.weights[0] + net.biases[0])\n",
    "        return emb\n",
    "    except:\n",
    "        return np.array([])\n",
    "    \n",
    "def get_similarity(word1, word2):\n",
    "    w1_emb = get_embeddings(word1)\n",
    "    w2_emb = get_embeddings(word2)\n",
    "    return cosine_similarity(w1_emb, w2_emb)\n",
    "    \n",
    "def get_result(word1, word2, word3):\n",
    "    w1_emb = get_embeddings(word1)\n",
    "    w2_emb = get_embeddings(word2)    \n",
    "    w3_emb = get_embeddings(word3)    \n",
    "    if(w1_emb.any() and w2_emb.any() and w3_emb.any()):\n",
    "        w4_emb = w1_emb + w3_emb - w2_emb\n",
    "        output = softmax(w4_emb @ net.weights[1] + net.biases[1])\n",
    "        ans = idx2word[np.argmax(output)]\n",
    "        return ans\n",
    "    return \"UNK\"\n",
    "\n",
    "def get_accuracy(validation):\n",
    "    accuracy = np.array([])\n",
    "    for _, row in validation.iterrows():\n",
    "        w1_emb = get_embeddings(row['word1'])\n",
    "        w2_emb = get_embeddings(row['word2'])    \n",
    "        w3_emb = get_embeddings(row['word3'])\n",
    "        w4_emb = get_embeddings(row['word4'])\n",
    "        \n",
    "        if(w1_emb.any() and w2_emb.any() and w3_emb.any()) and w4_emb.any():\n",
    "            pred_emb = w1_emb + w3_emb - w2_emb\n",
    "            accuracy = np.append(accuracy, cosine_similarity(pred_emb, w4_emb))\n",
    "    return np.average(accuracy)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=-1))\n",
    "\n",
    "\n",
    "def train(\n",
    "    net, optimizer, lamda, max_epochs, dev_input, dev_target, batch_size):\n",
    "    m = int(len(tokens)*0.8)\n",
    "\n",
    "    for e in range(max_epochs):\n",
    "        epoch_loss = 0.\n",
    "        for i in range(0, m, batch_size):\n",
    "            batch_input, batch_target = read_data(batch_size, i)\n",
    "            pred = net(batch_input)\n",
    "\n",
    "            # Compute gradients of loss w.r.t. weights and biases\n",
    "            dW, db = net.backward(batch_input, batch_target, lamda)\n",
    "\n",
    "            # Get updated weights based on current weights and gradients\n",
    "            weights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)\n",
    "\n",
    "            # Update model's weights and biases\n",
    "            net.weights = weights_updated\n",
    "            net.biases = biases_updated\n",
    "            print(e, i, cross_entropy_loss(pred, batch_target))\n",
    "\n",
    "        dev_pred = net(dev_input)\n",
    "        indices = np.argpartition(dev_pred, -1, axis=1)[:, -1:]\n",
    "        converted_matrix = np.zeros_like(dev_pred)\n",
    "        converted_matrix[np.arange(dev_pred.shape[0])[:, np.newaxis], indices] = 1\n",
    "        print('F1 Score on dev data: {:.5f}'.format(f1_score(dev_target, converted_matrix, average='micro')))\n",
    "\n",
    "def read_data(batch_size, index):\n",
    "    \n",
    "    # Initialize the input and output arrays\n",
    "    input_arr = np.zeros((batch_size, vocab_size), dtype=np.float32)\n",
    "    output_arr = np.zeros((batch_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "    # Loop over each word in the tokens list\n",
    "    if index + batch_size > len(tokens): k = len(tokens)\n",
    "    else: k = index + batch_size\n",
    "    for i in range(index, k):\n",
    "        # Get the context words\n",
    "        context_words = tokens[max(0, i - window_size): i] + tokens[i + 1: i + window_size + 1]\n",
    "        context_indices = [word2idx[w] for w in context_words]\n",
    "\n",
    "        # # Get the negative samples\n",
    "        negative_indices = np.random.choice(len(vocab), num_neg_samples, replace=False)\n",
    "        negative_indices = [idx for idx in negative_indices if idx not in context_indices]\n",
    "\n",
    "        # # Update the input and output arrays\n",
    "        input_arr[i - index, word2idx[tokens[i]]] = 1\n",
    "        for ctx_idx in context_indices:\n",
    "            output_arr[i - index, ctx_idx] = 1\n",
    "        for neg_idx in negative_indices:\n",
    "            output_arr[i - index, neg_idx] = 1\n",
    "    train_input = input_arr.astype(np.int32)\n",
    "    train_target = output_arr.astype(np.int32)\n",
    "\n",
    "\n",
    "    return train_target, train_input\n",
    "\n",
    "def dev_data():\n",
    "    # Initialize the input and output arrays\n",
    "    input_arr = np.zeros((int(len(tokens)*0.2) + 1, len(vocab)), dtype=np.float32)\n",
    "    output_arr = np.zeros((int(len(tokens)*0.2) + 1, len(vocab)), dtype=np.float32)\n",
    "    for i in range(int(len(tokens)*0.8), len(tokens)):\n",
    "        # Get the context words\n",
    "        context_words = tokens[max(0, i - window_size): i] + tokens[i + 1: i + window_size + 1]\n",
    "        context_indices = [word2idx[w] for w in context_words]\n",
    "\n",
    "        input_arr[i - int(len(tokens)*0.8), word2idx[tokens[i]]] = 1\n",
    "        for ctx_idx in context_indices:\n",
    "            output_arr[i - int(len(tokens)*0.8), ctx_idx] = 1\n",
    "    train_input = input_arr.astype(np.int32)\n",
    "    train_target = output_arr.astype(np.int32)\n",
    "    return train_target, train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "JM5S-M2aJGeN"
   },
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self):\n",
    "        if os.path.exists(WEIGHTS_PATH + 'cbow_biases_' + str(vocab_size) + '.pickle'):\n",
    "            with open(WEIGHTS_PATH + 'cbow_biases_' + str(vocab_size) + '.pickle', \"rb\") as f:\n",
    "                self.weights = pickle.load(f)\n",
    "            with open(WEIGHTS_PATH + 'cbow_biases_' + str(vocab_size) + '.pickle', \"rb\") as f:\n",
    "                self.biases = pickle.load(f)\n",
    "        else:\n",
    "            self.weights = [np.random.randn(vocab_size, embedding_size) / np.sqrt(embedding_size)]\n",
    "            self.weights.append(np.random.randn(embedding_size, vocab_size) / np.sqrt(embedding_size))\n",
    "            self.biases = [np.random.randn(embedding_size) / np.sqrt(embedding_size)]\n",
    "            self.biases.append(np.random.randn(vocab_size) / np.sqrt(embedding_size))\n",
    "\n",
    "    def __call__(self, X):\n",
    "        a = (X @ self.weights[0] + self.biases[0])\n",
    "        output = softmax(a @ self.weights[1] + self.biases[1])\n",
    "        return output\n",
    "\n",
    "    def backward(self, X, y, lamda):\n",
    "        batch_size = len(X)\n",
    "        a = (X @ self.weights[0] + self.biases[0])\n",
    "        output = softmax(a @ self.weights[1] + self.biases[1])\n",
    "        del_W = [np.zeros(w.shape) for w in self.weights]\n",
    "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        delta = output - y\n",
    "        del_b[1] = np.sum(delta, axis = 0, keepdims = True)\n",
    "        del_W[1] = a.T @ delta + lamda * (self.weights[1])\n",
    "\n",
    "        delta = delta @ self.weights[1].T * (a)\n",
    "        del_b[0] = np.sum(delta, axis = 0, keepdims = True)\n",
    "        del_W[0] = X.T @ delta + lamda * (self.weights[0])\n",
    "        return del_W, del_b\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, learning_rate, weights, biases):\n",
    "        self.m_dw = [np.zeros(w.shape) for w in weights]\n",
    "        self.m_db = [np.zeros(b.shape) for b in biases]\n",
    "        self.v_dw = [np.zeros(w.shape) for w in weights]\n",
    "        self.v_db = [np.zeros(b.shape) for b in biases]\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.eta = learning_rate\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, weights, biases, delta_weights, delta_biases):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_dw = [self.beta1 * m + (1 - self.beta1) * del_w for m, del_w in zip(self.m_dw, delta_weights)]\n",
    "        self.m_db = [self.beta1 * m + (1 - self.beta1) * del_b for m, del_b in zip(self.m_db, delta_biases)]\n",
    "        self.v_dw = [self.beta2 * v + (1 - self.beta2) * (del_w**2) for v, del_w in zip(self.v_dw, delta_weights)]\n",
    "        self.v_db = [self.beta2 * v + (1 - self.beta2) * (del_b**2) for v, del_b in zip(self.v_db, delta_biases)]\n",
    "\n",
    "        # bias correction\n",
    "        m_hat_dw = [m / (1 - self.beta1 ** self.t) for m in self.m_dw]\n",
    "        v_hat_dw = [v / (1 - self.beta2 ** self.t) for v in self.v_dw]\n",
    "\n",
    "        m_hat_db = [m / (1 - self.beta1 ** self.t) for m in self.m_db]\n",
    "        v_hat_db = [v / (1 - self.beta2 ** self.t) for v in self.v_db]\t\n",
    "\n",
    "        # update weights and biases\n",
    "        weights = [w - self.eta * m_hat / ((np.sqrt(v_hat) + self.epsilon)) for w, m_hat, v_hat in zip(weights, m_hat_dw, v_hat_dw)] \n",
    "        biases = [b - self.eta * m_hat / ((np.sqrt(v_hat) + self.epsilon)) for b, m_hat, v_hat in zip(biases, m_hat_db, v_hat_db)]\n",
    "\n",
    "        return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zW9jaMrXJsDp",
    "outputId": "cfabbc75-1857-428e-8af3-54ee27145495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 8.551934865338378\n",
      "0 16192 7.956681470743348\n",
      "F1 Score on dev data: 0.05455\n",
      "1 0 7.54775618994377\n",
      "1 16192 8.125862748326966\n",
      "F1 Score on dev data: 0.11574\n",
      "2 0 6.431450885334984\n",
      "2 16192 5.662905938491707\n",
      "F1 Score on dev data: 0.17860\n",
      "3 0 4.682940031758605\n",
      "3 16192 4.241846158073566\n",
      "F1 Score on dev data: 0.23602\n",
      "4 0 3.6918613236441082\n",
      "4 16192 3.6437318248218014\n",
      "F1 Score on dev data: 0.29556\n",
      "5 0 3.1823407418542407\n",
      "5 16192 3.5199231430059412\n",
      "F1 Score on dev data: 0.27667\n",
      "6 0 3.4096018553335874\n",
      "6 16192 3.6825483603646805\n",
      "F1 Score on dev data: 0.29601\n",
      "7 0 3.8305430963804534\n",
      "7 16192 3.922825709002432\n",
      "F1 Score on dev data: 0.31006\n",
      "8 0 4.132589193531979\n",
      "8 16192 4.074528880526385\n",
      "F1 Score on dev data: 0.31596\n",
      "9 0 4.5136348736679075\n",
      "9 16192 4.884191046286319\n",
      "F1 Score on dev data: 0.31520\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters \n",
    "max_epochs = 10\n",
    "learning_rate = 0.01\n",
    "lamda = 1 # Regularization Parameter\n",
    "batch_size = 16192\n",
    "\n",
    "\n",
    "net = Net()\n",
    "optimizer = Optimizer(learning_rate, net.weights, net.biases)\n",
    "dev_input, dev_target = dev_data()\n",
    "train(net, optimizer, lamda, max_epochs, dev_input, dev_target, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WEIGHTS_PATH + 'cbow_weights_' + str(vocab_size) + '_' + str(embedding_size) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(net.weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(WEIGHTS_PATH + 'cbow_biases_' + str(vocab_size) + '_' + str(embedding_size) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(net.biases, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.9596947795349271\n",
      "Coverage is : 0.6952573158425832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word1, word2, word3, word4, result]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = pd.read_csv('./../data/Validation.txt', sep=' ', names=['word1','word2','word3','word4'])\n",
    "\n",
    "validation['word1'] = validation['word1'].apply(lambda x : x.lower())\n",
    "validation['word2'] = validation['word2'].apply(lambda x : x.lower())\n",
    "validation['word3'] = validation['word3'].apply(lambda x : x.lower())\n",
    "validation['word4'] = validation['word4'].apply(lambda x : x.lower())\n",
    "\n",
    "validation['result'] = validation[['word1','word2','word3']].apply(lambda x : get_result(x['word1'], x['word2'], x['word3']), axis = 1)\n",
    "\n",
    "print(\"Accuracy is :\", get_accuracy(validation))\n",
    "print(\"Coverage is :\", validation[validation['result']!=\"\"].shape[0]/991)\n",
    "validation[validation['result']==validation['word4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = validation[validation['result']!=\"\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>see</td>\n",
       "      <td>sees</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>shuffle</td>\n",
       "      <td>shuffles</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sing</td>\n",
       "      <td>sings</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sit</td>\n",
       "      <td>sits</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>slow</td>\n",
       "      <td>slows</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>naira</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>japan</td>\n",
       "      <td>yen</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>denmark</td>\n",
       "      <td>krone</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1  word2    word3     word4 result\n",
       "0         walk  walks      see      sees   must\n",
       "1         walk  walks  shuffle  shuffles   must\n",
       "2         walk  walks     sing     sings   must\n",
       "3         walk  walks      sit      sits   must\n",
       "4         walk  walks     slow     slows   must\n",
       "..         ...    ...      ...       ...    ...\n",
       "986  argentina   peso  nigeria     naira    man\n",
       "987  argentina   peso     iran      rial  world\n",
       "988  argentina   peso    japan       yen  world\n",
       "989      india  rupee     iran      rial  world\n",
       "990      india  rupee  denmark     krone  world\n",
       "\n",
       "[991 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.to_csv(\"./../data/results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'king'\n",
    "word2 = 'man'\n",
    "word3 = 'queen'\n",
    "\n",
    "print(get_result('king','man','queen'))\n",
    "\n",
    "get_similarity('queen', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.977057841861813\n",
      "Coverage is : 0.05247225025227043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word1, word2, word3, word4, result]\n",
       "Index: []"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = pd.read_csv('./../data/Analogy_dataset.txt', sep=' ', names=['word1','word2','word3','word4'])\n",
    "\n",
    "validation['word1'] = validation['word1'].apply(lambda x : x.lower())\n",
    "validation['word2'] = validation['word2'].apply(lambda x : x.lower())\n",
    "validation['word3'] = validation['word3'].apply(lambda x : x.lower())\n",
    "validation['word4'] = validation['word4'].apply(lambda x : x.lower())\n",
    "\n",
    "validation['result'] = validation[['word1','word2','word3']].apply(lambda x : get_result(x['word1'], x['word2'], x['word3']), axis = 1)\n",
    "\n",
    "print(\"Accuracy is :\", get_accuracy(validation))\n",
    "print(\"Coverage is :\", validation[validation['result']!=\"\"].shape[0]/991)\n",
    "validation[validation['result']==validation['word4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ePUwGUW5IAq9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     324\n",
      "False      2\n",
      "Name: w1_present, dtype: int64\n",
      "True     299\n",
      "False     27\n",
      "Name: w2_present, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./../data/Validation.txt', sep=' ', names=['w1','w2','w3','w4'])\n",
    "df2 = df[['w3','w4']].rename(columns={'w3':'w1', 'w4':'w2'})\n",
    "df = pd.concat([df[['w1','w2']],df2], ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['w1'] = df['w1'].apply(lambda x : x.lower())\n",
    "df['w2'] = df['w2'].apply(lambda x : x.lower())\n",
    "\n",
    "df['w1_present'] = df['w1'].apply(lambda x : x in result)\n",
    "df['w2_present'] = df['w2'].apply(lambda x : x in result)\n",
    "\n",
    "print(df['w1_present'].value_counts())\n",
    "print(df['w2_present'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('weights.pickle', 'wb') as handle:\n",
    "    pickle.dump(net.weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('biases.pickle', 'wb') as handle:\n",
    "    pickle.dump(net.biases, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
