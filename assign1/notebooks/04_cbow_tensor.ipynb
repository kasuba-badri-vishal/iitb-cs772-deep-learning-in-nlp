{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afa00d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "text_counter = Counter()\n",
    "tqdm.pandas()\n",
    "mps_device = torch.device(\"mps\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8aab0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = './../weights/'\n",
    "DATASET_PATH = './../data/processed/processed_data.csv'\n",
    "INTERMEDIATE_PATH = './../intermediate/'\n",
    "RESULTS_PATH = './../results/'\n",
    "VALIDATION = './../data/Validation.txt'\n",
    "\n",
    "\n",
    "embedding_size = 512\n",
    "no_of_rows     = 30000\n",
    "window_size    = 4\n",
    "max_epochs     = 10\n",
    "learning_rate  = 0.001\n",
    "reg_lambda     = 1\n",
    "batch_size     = 2048\n",
    "train_split    = 0.8\n",
    "is_train       = True\n",
    "new_weights    = True\n",
    "use_biases     = False\n",
    "problem        = 'cbow'\n",
    "\n",
    "epochs_stat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12914f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.emb    = None\n",
    "\n",
    "        if(os.path.exists(WEIGHTS_PATH + 'cbow_weights_' + str(vocab_size) + '.pt') and (new_weights == False)):\n",
    "            self.weights = torch.load(WEIGHTS_PATH + 'cbow_weights_' + str(vocab_size) + '.pt')\n",
    "            self.biases = torch.load(WEIGHTS_PATH + 'cbow_biases_' + str(vocab_size) + '.pt')\n",
    "        else:\n",
    "            self.weights = []\n",
    "            self.biases  = []\n",
    "            \n",
    "            self.weights.append(torch.rand((vocab_size, embedding_size), device=mps_device) * 2 - 1)\n",
    "            self.weights.append(torch.rand((embedding_size, vocab_size), device=mps_device) * 2 - 1)\n",
    "            if(use_biases):\n",
    "                self.biases.append(torch.rand((embedding_size), device=mps_device) * 2 - 1)\n",
    "                self.biases.append(torch.rand((vocab_size), device=mps_device) * 2 - 1)\n",
    "            else:\n",
    "                self.biases.append(torch.zeros((embedding_size), device=mps_device))\n",
    "                self.biases.append(torch.zeros((vocab_size), device=mps_device))\n",
    "\n",
    "    def __call__(self, X):\n",
    "        self.emb = (torch.matmul(X,self.weights[0]) + self.biases[0])\n",
    "        self.y_pred = torch.softmax(torch.matmul(self.emb, self.weights[1]) + self.biases[1], dim=1)\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, X, y, lamda):\n",
    "        \n",
    "        del_W = []\n",
    "        del_b = []\n",
    "\n",
    "        delta = self.y_pred - y\n",
    "        del_b.insert(0,torch.sum(delta, axis = 0, keepdims = True))\n",
    "        del_W.insert(0,torch.matmul(self.emb.T, delta) + lamda * (self.weights[1]))\n",
    "\n",
    "        delta = torch.matmul(delta, self.weights[1].T) * (self.emb)\n",
    "        del_b.insert(0,torch.sum(delta, axis = 0, keepdims = True))\n",
    "        del_W.insert(0,torch.matmul(X.T, delta) + lamda * (self.weights[0]))\n",
    "        return del_W, del_b\n",
    "\n",
    "    \n",
    "class Optimizer(object):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, learning_rate, weights, biases, optimizer=\"gradient\"):\n",
    "        \n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.m_dw = [torch.zeros((w.shape), device=mps_device) for w in weights]\n",
    "        self.m_db = [torch.zeros((b.shape), device=mps_device) for b in biases]\n",
    "        self.v_dw = [torch.zeros((w.shape), device=mps_device) for w in weights]\n",
    "        self.v_db = [torch.zeros((b.shape), device=mps_device) for b in biases]\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.eta = learning_rate\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, weights, biases, delta_weights, delta_biases):\n",
    "        \n",
    "        if(self.optimizer == \"gradient\"):\n",
    "            return self.gradient(weights, biases, delta_weights, delta_biases)\n",
    "        elif(self.optimizer == \"adam\"):\n",
    "            return self.adam(weights, biases, delta_weights, delta_biases)\n",
    "    \n",
    "    def adam(self, weights, biases, delta_weights, delta_biases):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_dw = [self.beta1 * m + (1 - self.beta1) * del_w for m, del_w in zip(self.m_dw, delta_weights)]\n",
    "        self.m_db = [self.beta1 * m + (1 - self.beta1) * del_b for m, del_b in zip(self.m_db, delta_biases)]\n",
    "        self.v_dw = [self.beta2 * v + (1 - self.beta2) * (del_w**2) for v, del_w in zip(self.v_dw, delta_weights)]\n",
    "        self.v_db = [self.beta2 * v + (1 - self.beta2) * (del_b**2) for v, del_b in zip(self.v_db, delta_biases)]\n",
    "\n",
    "        # bias correction\n",
    "        m_hat_dw = [m / (1 - self.beta1 ** self.t) for m in self.m_dw]\n",
    "        v_hat_dw = [v / (1 - self.beta2 ** self.t) for v in self.v_dw]\n",
    "\n",
    "        m_hat_db = [m / (1 - self.beta1 ** self.t) for m in self.m_db]\n",
    "        v_hat_db = [v / (1 - self.beta2 ** self.t) for v in self.v_db]\n",
    "\n",
    "        # update weights and biases\n",
    "        weights = [w - self.eta * m_hat / ((torch.sqrt(v_hat) + self.epsilon)) for w, m_hat, v_hat in zip(weights, m_hat_dw, v_hat_dw)] \n",
    "        biases = [b - self.eta * m_hat / ((torch.sqrt(v_hat) + self.epsilon)) for b, m_hat, v_hat in zip(biases, m_hat_db, v_hat_db)]\n",
    "        return weights, biases\n",
    "\n",
    "    \n",
    "    def gradient(self, weights, biases, delta_weights, delta_biases):\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i] - self.eta*delta_weights[i]\n",
    "            biases[i] = biases[i] - self.eta*delta_biases[i]\n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "56270eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word):\n",
    "    try:\n",
    "        val = word2idx[word]\n",
    "        input_arr = torch.zeros((vocab_size), dtype=torch.float, device=mps_device)\n",
    "        input_arr[val] = 1\n",
    "        emb = torch.matmul(input_arr,net.weights[0]) + net.biases[0]\n",
    "        return emb\n",
    "    except:\n",
    "        return torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "\n",
    "def get_result(word1, word2, word3, problem='skipgram', window=4):\n",
    "    w1_emb = get_embeddings(word1)\n",
    "    w2_emb = get_embeddings(word2)    \n",
    "    w3_emb = get_embeddings(word3)    \n",
    "    if((torch.numel(w1_emb) != 0) and (torch.numel(w2_emb) != 0) and (torch.numel(w3_emb) != 0)):\n",
    "        w4_emb = w1_emb + w3_emb - w2_emb\n",
    "        output = torch.softmax(torch.matmul(w4_emb, net.weights[1]) + net.biases[1], dim=1)\n",
    "        if(problem=='skipgram'):\n",
    "            topk_preds = torch.topk(output, k=window).indices.tolist()\n",
    "            ans = [idx2word[x] for x in topk_preds[0]]\n",
    "        else:\n",
    "            ans = idx2word[torch.argmax(output).item()]\n",
    "        return ans\n",
    "    return \"UNK\"\n",
    "\n",
    "\n",
    "def get_embeddings(word):\n",
    "    try:\n",
    "        val = word2idx[word]\n",
    "        input_arr = torch.zeros((vocab_size), dtype=torch.float, device=mps_device)\n",
    "        input_arr[val] = 1\n",
    "        emb = torch.matmul(input_arr,net.weights[0]) + net.biases[0]\n",
    "        return emb\n",
    "    except:\n",
    "        return torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "    \n",
    "def get_accuracy(validation):\n",
    "    accuracy = torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "    for _, row in validation.iterrows():\n",
    "        w1_emb = get_embeddings(row['word1'])\n",
    "        w2_emb = get_embeddings(row['word2'])    \n",
    "        w3_emb = get_embeddings(row['word3'])\n",
    "        w4_emb = get_embeddings(row['word4'])\n",
    "        \n",
    "        if((torch.numel(w1_emb) != 0) and (torch.numel(w2_emb) != 0) and (torch.numel(w3_emb) != 0)):\n",
    "            pred_emb = w1_emb + w3_emb - w2_emb\n",
    "            accuracy = torch.cat(accuracy, cosine_similarity(pred_emb, w4_emb), dim=0)\n",
    "    return torch.mean(accuracy, dim=0)\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    C = torch.squeeze(A)\n",
    "    D = torch.squeeze(B)\n",
    "    return torch.dot(C, D) / (torch.norm(C) * torch.norm(D))\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    if(mps_device.type == 'mps'):\n",
    "        return -torch.mean(custom_nansum(y_true * torch.log(y_pred), axis=-1))\n",
    "    else:\n",
    "        return -torch.mean(torch.nansum(y_true * torch.log(y_pred), axis=-1))\n",
    "\n",
    "def get_dataset(words, window_size):\n",
    "    data = pd.DataFrame(columns=[\"word\", \"context_words\"])\n",
    "    for index, word in enumerate(words):\n",
    "        context_words = words[max(0, index - window_size): index] + words[index + 1: index + window_size + 1]\n",
    "        context_words = list(set(context_words))\n",
    "        data.loc[len(data.index)] = [word, context_words]\n",
    "    global dataset\n",
    "    dataset = pd.concat([dataset,data])\n",
    "    \n",
    "def custom_nansum(input_tensor, axis=None):\n",
    "    # Replace NaN values with 0\n",
    "    input_tensor = torch.where(torch.isnan(input_tensor), torch.tensor(0., device=input_tensor.device), input_tensor)\n",
    "    \n",
    "    # Compute the sum over the specified axis\n",
    "    if axis is None:\n",
    "        return torch.sum(input_tensor)\n",
    "    else:\n",
    "        return torch.sum(input_tensor, dim=axis)\n",
    "    \n",
    "    \n",
    "def get_batch_data(start_index, end_index):\n",
    "\n",
    "    batch_dataset = dataset[start_index:end_index].reset_index(drop=True)\n",
    "    batch_size = batch_dataset.shape[0]\n",
    "    batch_input = torch.zeros([batch_size, vocab_size], dtype=torch.float, device=mps_device)\n",
    "    batch_output = torch.zeros([batch_size, vocab_size], dtype=torch.float, device=mps_device)\n",
    "\n",
    "    for index, data in batch_dataset.iterrows():\n",
    "        batch_input[index, data['word']] = 1\n",
    "        for ind in data['context_words']:\n",
    "            batch_output[index, ind] = 1\n",
    "            \n",
    "    return batch_output, batch_input\n",
    "\n",
    "\n",
    "def train(net, optimizer, lamda, max_epochs, dev_input, dev_target, batch_size, train_size):\n",
    "\n",
    "    stop = False\n",
    "    value = 999999999\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        first_loss = True\n",
    "        epoch = {}\n",
    "        batches = []\n",
    "        losses = []\n",
    "        for start_index in range(0, train_size, batch_size):\n",
    "            end_index = min(start_index + batch_size, train_size)\n",
    "            batch_target, batch_input = get_batch_data(start_index, end_index)\n",
    "            pred = net(batch_input)\n",
    "\n",
    "            # Compute gradients of loss w.r.t. weights and biases\n",
    "            dW, db = net.backward(batch_input, batch_target, lamda)\n",
    "\n",
    "            # Get updated weights based on current weights and gradients\n",
    "            weights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)\n",
    "\n",
    "            # Update model's weights and biases\n",
    "            net.weights = weights_updated\n",
    "            net.biases = biases_updated\n",
    "            loss = cross_entropy_loss(pred, batch_target)\n",
    "            print(e, start_index, loss.item())\n",
    "            batches.append(start_index)\n",
    "            losses.append(loss.item())\n",
    "            if(torch.isnan(loss) or torch.isinf(loss)):\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "            if(first_loss):\n",
    "                first_loss = False\n",
    "                if(value<loss.item()):\n",
    "                    stop = True\n",
    "                    break\n",
    "                else:\n",
    "                    value = loss.item()\n",
    "\n",
    "        epoch['batches'] = batches\n",
    "        epoch['losses'] = losses\n",
    "            \n",
    "\n",
    "        dev_pred = net(dev_input)\n",
    "        indices = torch.topk(dev_pred, k=window_size, dim=1)[1][:, -window_size:]\n",
    "        converted_matrix = torch.zeros_like(dev_pred)\n",
    "        converted_matrix[torch.arange(dev_pred.shape[0])[:, None], indices] = 1\n",
    "        numpy_dev_target = dev_target.cpu().numpy()\n",
    "        converted_matrix = converted_matrix.cpu().numpy()\n",
    "        score = f1_score(numpy_dev_target, converted_matrix, average='micro')\n",
    "        print('F1 Score on dev data: {:.5f}'.format(score))\n",
    "        epoch['f1_score'] = score\n",
    "        epochs_stat.append(epoch)\n",
    "        if(stop):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec17f0",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5f657279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607836\n",
      "Vocab length : 40107\n",
      "Dataset size : 216522\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH ,nrows = no_of_rows)\n",
    "df = df.dropna()\n",
    "\n",
    "df['sentences'] = df['sentences'].apply(lambda sentence : sentence.split())\n",
    "_ = df['sentences'].apply(text_counter.update)\n",
    "text_counter.update(['UNK'])\n",
    "vocab_size = len(text_counter)\n",
    "\n",
    "print(sum(text_counter.values()))\n",
    "\n",
    "words, _ = zip(*text_counter.most_common(vocab_size))\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "idx2word = {i: w for i, w in enumerate(words)}\n",
    "\n",
    "\n",
    "# dataset = pd.read_pickle('dataset.pkl')\n",
    "df['sentences'] = df['sentences'].apply(lambda words : [word2idx[word] for word in words])\n",
    "\n",
    "if(no_of_rows==None and os.path.exists(INTERMEDIATE_PATH + 'dataset.pkl')):\n",
    "   dataset = pd.read_pickle(INTERMEDIATE_PATH + 'dataset.pkl') \n",
    "\n",
    "elif(os.path.exists(INTERMEDIATE_PATH + 'dataset_'+str(no_of_rows)+'.pkl')):\n",
    "    dataset = pd.read_pickle(INTERMEDIATE_PATH + 'dataset_'+str(no_of_rows)+'.pkl')\n",
    "else:\n",
    "    dataset = pd.DataFrame(columns=[\"word\", \"context_words\"])\n",
    "    _ = df['sentences'].progress_apply(lambda x : get_dataset(x, window_size))\n",
    "\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    if(no_of_rows==None):\n",
    "        dataset.to_pickle(INTERMEDIATE_PATH + 'dataset.pkl') \n",
    "    else:\n",
    "        dataset.to_pickle(INTERMEDIATE_PATH + 'dataset_'+str(no_of_rows)+'.pkl')\n",
    "\n",
    "data_size = dataset.shape[0]\n",
    "\n",
    "print(\"Vocab length :\", vocab_size)\n",
    "print(\"Dataset size :\", data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a02f8",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "061fde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = max(int(0.90*data_size), data_size-5000)\n",
    "\n",
    "net = Net()\n",
    "optimizer = Optimizer(learning_rate, net.weights, net.biases, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "88fbf234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 139.7922821044922\n",
      "0 2048 136.05772399902344\n",
      "0 4096 137.91940307617188\n",
      "0 6144 135.83111572265625\n",
      "0 8192 135.20233154296875\n",
      "0 10240 136.98550415039062\n",
      "0 12288 135.4839324951172\n",
      "0 14336 133.4728240966797\n",
      "0 16384 133.40158081054688\n",
      "0 18432 133.43368530273438\n",
      "0 20480 131.66600036621094\n",
      "0 22528 131.90745544433594\n",
      "0 24576 131.57225036621094\n",
      "0 26624 130.16317749023438\n",
      "0 28672 131.12863159179688\n",
      "0 30720 129.32839965820312\n",
      "0 32768 131.1597137451172\n",
      "0 34816 129.59036254882812\n",
      "0 36864 127.76079559326172\n",
      "0 38912 128.76026916503906\n",
      "0 40960 127.50633239746094\n",
      "0 43008 126.96383666992188\n",
      "0 45056 129.4069061279297\n",
      "0 47104 126.49707794189453\n",
      "0 49152 127.63768005371094\n",
      "0 51200 126.14712524414062\n",
      "0 53248 126.30921173095703\n",
      "0 55296 125.26234436035156\n",
      "0 57344 127.58151245117188\n",
      "0 59392 125.19808959960938\n",
      "0 61440 122.41328430175781\n",
      "0 63488 122.55841064453125\n",
      "0 65536 125.22294616699219\n",
      "0 67584 123.24613189697266\n",
      "0 69632 125.43917083740234\n",
      "0 71680 123.994140625\n",
      "0 73728 124.2583236694336\n",
      "0 75776 123.20292663574219\n",
      "0 77824 122.18557739257812\n",
      "0 79872 120.671875\n",
      "0 81920 121.77053833007812\n",
      "0 83968 121.70176696777344\n",
      "0 86016 121.41793060302734\n",
      "0 88064 119.845458984375\n",
      "0 90112 118.98165893554688\n",
      "0 92160 119.95564270019531\n",
      "0 94208 118.17001342773438\n",
      "0 96256 119.37531280517578\n",
      "0 98304 118.21420288085938\n",
      "0 100352 119.95353698730469\n",
      "0 102400 117.04718017578125\n",
      "0 104448 117.53260803222656\n",
      "0 106496 116.97746276855469\n",
      "0 108544 113.99856567382812\n",
      "0 110592 116.25933837890625\n",
      "0 112640 116.06340026855469\n",
      "0 114688 116.38954162597656\n",
      "0 116736 114.71324157714844\n",
      "0 118784 114.47676086425781\n",
      "0 120832 115.23078155517578\n",
      "0 122880 114.67758178710938\n",
      "0 124928 112.96183013916016\n",
      "0 126976 113.64524841308594\n",
      "0 129024 111.8673095703125\n",
      "0 131072 113.57467651367188\n",
      "0 133120 110.52308654785156\n",
      "0 135168 113.1912841796875\n",
      "0 137216 114.00955200195312\n",
      "0 139264 112.16410064697266\n",
      "0 141312 112.03085327148438\n",
      "0 143360 113.8235855102539\n",
      "0 145408 111.03370666503906\n",
      "0 147456 110.49646759033203\n",
      "0 149504 110.9691162109375\n",
      "0 151552 108.79299926757812\n",
      "0 153600 109.40737915039062\n",
      "0 155648 110.05230712890625\n",
      "0 157696 109.41709899902344\n",
      "0 159744 108.76459503173828\n",
      "0 161792 108.35099029541016\n",
      "0 163840 108.03992462158203\n",
      "0 165888 108.16403198242188\n",
      "0 167936 106.56983184814453\n",
      "0 169984 108.19734191894531\n",
      "0 172032 107.69124603271484\n",
      "0 174080 108.26394653320312\n",
      "0 176128 106.63236236572266\n",
      "0 178176 106.85515594482422\n",
      "0 180224 105.33535766601562\n",
      "0 182272 105.87015533447266\n",
      "0 184320 106.55123901367188\n",
      "0 186368 105.44137573242188\n",
      "0 188416 106.95663452148438\n",
      "0 190464 105.26026916503906\n",
      "0 192512 104.88937377929688\n",
      "0 194560 103.97412872314453\n",
      "0 196608 104.53730773925781\n",
      "0 198656 101.7648696899414\n",
      "0 200704 104.40049743652344\n",
      "0 202752 103.03124237060547\n",
      "0 204800 103.48602294921875\n",
      "0 206848 102.93927001953125\n",
      "0 208896 101.10983276367188\n",
      "0 210944 103.80326843261719\n",
      "F1 Score on dev data: 0.00717\n",
      "1 0 102.72969055175781\n",
      "1 2048 100.59446716308594\n",
      "1 4096 101.75306701660156\n",
      "1 6144 100.49111938476562\n",
      "1 8192 100.03353881835938\n",
      "1 10240 101.9449462890625\n",
      "1 12288 100.86226654052734\n",
      "1 14336 99.38983154296875\n",
      "1 16384 99.43350219726562\n",
      "1 18432 99.65608215332031\n",
      "1 20480 97.984619140625\n",
      "1 22528 98.65321350097656\n",
      "1 24576 98.25990295410156\n",
      "1 26624 97.05342102050781\n",
      "1 28672 98.1103515625\n",
      "1 30720 96.64918518066406\n",
      "1 32768 98.36582946777344\n",
      "1 34816 97.09443664550781\n",
      "1 36864 95.83439636230469\n",
      "1 38912 96.50272369384766\n",
      "1 40960 95.68327331542969\n",
      "1 43008 95.37818908691406\n",
      "1 45056 97.27237701416016\n",
      "1 47104 94.96936798095703\n",
      "1 49152 95.79026794433594\n",
      "1 51200 94.90602111816406\n",
      "1 53248 95.08492279052734\n",
      "1 55296 94.50884246826172\n",
      "1 57344 96.2630615234375\n",
      "1 59392 94.48892211914062\n",
      "1 61440 92.3132095336914\n",
      "1 63488 92.24124145507812\n",
      "1 65536 94.466552734375\n",
      "1 67584 92.96351623535156\n",
      "1 69632 94.58687591552734\n",
      "1 71680 93.76615905761719\n",
      "1 73728 94.01248168945312\n",
      "1 75776 93.38233947753906\n",
      "1 77824 92.61592864990234\n",
      "1 79872 91.35650634765625\n",
      "1 81920 92.27154541015625\n",
      "1 83968 92.24117279052734\n",
      "1 86016 91.86527252197266\n",
      "1 88064 90.86297607421875\n",
      "1 90112 90.04708862304688\n",
      "1 92160 91.05213165283203\n",
      "1 94208 89.62107849121094\n",
      "1 96256 90.46630859375\n",
      "1 98304 89.88917541503906\n",
      "1 100352 91.33241271972656\n",
      "1 102400 89.16731262207031\n",
      "1 104448 89.68173217773438\n",
      "1 106496 89.0466079711914\n",
      "1 108544 87.01753997802734\n",
      "1 110592 88.3196792602539\n",
      "1 112640 88.75192260742188\n",
      "1 114688 89.05868530273438\n",
      "1 116736 87.84476470947266\n",
      "1 118784 87.57218170166016\n",
      "1 120832 88.084716796875\n",
      "1 122880 87.85675048828125\n",
      "1 124928 86.46511840820312\n",
      "1 126976 87.34390258789062\n",
      "1 129024 85.39043426513672\n",
      "1 131072 87.1506118774414\n",
      "1 133120 84.88249206542969\n",
      "1 135168 86.89091491699219\n",
      "1 137216 87.55751037597656\n",
      "1 139264 85.96012115478516\n",
      "1 141312 85.98745727539062\n",
      "1 143360 87.73460388183594\n",
      "1 145408 85.32867431640625\n",
      "1 147456 85.17608642578125\n",
      "1 149504 85.9588851928711\n",
      "1 151552 84.17059326171875\n",
      "1 153600 84.80155181884766\n",
      "1 155648 85.21240234375\n",
      "1 157696 85.14996337890625\n",
      "1 159744 84.30477905273438\n",
      "1 161792 84.00524139404297\n",
      "1 163840 83.97312927246094\n",
      "1 165888 84.36742401123047\n",
      "1 167936 82.87705993652344\n",
      "1 169984 84.40887451171875\n",
      "1 172032 84.04610443115234\n",
      "1 174080 84.76704406738281\n",
      "1 176128 83.3719253540039\n",
      "1 178176 83.43732452392578\n",
      "1 180224 82.62879180908203\n",
      "1 182272 83.18377685546875\n",
      "1 184320 83.89620971679688\n",
      "1 186368 83.07760620117188\n",
      "1 188416 84.26628875732422\n",
      "1 190464 83.37423706054688\n",
      "1 192512 82.85686492919922\n",
      "1 194560 82.27247619628906\n",
      "1 196608 82.80560302734375\n",
      "1 198656 80.5130386352539\n",
      "1 200704 82.95498657226562\n",
      "1 202752 81.6160659790039\n",
      "1 204800 82.16349792480469\n",
      "1 206848 81.77964782714844\n",
      "1 208896 80.2426986694336\n",
      "1 210944 82.85153198242188\n",
      "F1 Score on dev data: 0.01458\n",
      "2 0 82.21174621582031\n",
      "2 2048 80.81377410888672\n",
      "2 4096 81.36648559570312\n",
      "2 6144 80.78466033935547\n",
      "2 8192 80.63330078125\n",
      "2 10240 82.5843734741211\n",
      "2 12288 81.78099060058594\n",
      "2 14336 80.38335418701172\n",
      "2 16384 80.18862915039062\n",
      "2 18432 80.93485260009766\n",
      "2 20480 79.35487365722656\n",
      "2 22528 80.1343765258789\n",
      "2 24576 80.1275405883789\n",
      "2 26624 79.19566345214844\n",
      "2 28672 79.89214324951172\n",
      "2 30720 78.9013671875\n",
      "2 32768 80.44117736816406\n",
      "2 34816 79.37285614013672\n",
      "2 36864 78.79155731201172\n",
      "2 38912 79.34834289550781\n",
      "2 40960 78.6423110961914\n",
      "2 43008 78.73004150390625\n",
      "2 45056 80.16090393066406\n",
      "2 47104 78.58839416503906\n",
      "2 49152 79.0327377319336\n",
      "2 51200 78.4537353515625\n",
      "2 53248 78.8140869140625\n",
      "2 55296 78.67012023925781\n",
      "2 57344 79.79664611816406\n",
      "2 59392 78.77639770507812\n",
      "2 61440 76.92729187011719\n",
      "2 63488 77.11990356445312\n",
      "2 65536 78.963623046875\n",
      "2 67584 77.73173522949219\n",
      "2 69632 79.3858642578125\n",
      "2 71680 78.91307067871094\n",
      "2 73728 78.8104248046875\n",
      "2 75776 78.37935638427734\n",
      "2 77824 78.27685546875\n",
      "2 79872 77.04534912109375\n",
      "2 81920 78.01914978027344\n",
      "2 83968 77.89512634277344\n",
      "2 86016 77.68607330322266\n",
      "2 88064 76.8009033203125\n",
      "2 90112 76.27299499511719\n",
      "2 92160 77.33357238769531\n",
      "2 94208 76.213623046875\n",
      "2 96256 76.89852905273438\n",
      "2 98304 76.81581115722656\n",
      "2 100352 77.764892578125\n",
      "2 102400 76.37884521484375\n",
      "2 104448 76.48420715332031\n",
      "2 106496 76.21141052246094\n",
      "2 108544 74.67799377441406\n",
      "2 110592 75.61056518554688\n",
      "2 112640 76.19137573242188\n",
      "2 114688 76.83308410644531\n",
      "2 116736 75.77738189697266\n",
      "2 118784 75.0744400024414\n",
      "2 120832 76.03446197509766\n",
      "2 122880 75.85733032226562\n",
      "2 124928 74.57061767578125\n",
      "2 126976 75.43327331542969\n",
      "2 129024 73.61293029785156\n",
      "2 131072 75.34803771972656\n",
      "2 133120 73.51940155029297\n",
      "2 135168 75.12025451660156\n",
      "2 137216 75.81411743164062\n",
      "2 139264 74.35302734375\n",
      "2 141312 74.48738861083984\n",
      "2 143360 76.12593078613281\n",
      "2 145408 74.01162719726562\n",
      "2 147456 74.10659790039062\n",
      "2 149504 74.85704040527344\n",
      "2 151552 73.1940689086914\n",
      "2 153600 73.94131469726562\n",
      "2 155648 74.16749572753906\n",
      "2 157696 74.54486083984375\n",
      "2 159744 73.47161865234375\n",
      "2 161792 73.0560302734375\n",
      "2 163840 73.39724731445312\n",
      "2 165888 73.74464416503906\n",
      "2 167936 72.67555236816406\n",
      "2 169984 73.73184204101562\n",
      "2 172032 73.57931518554688\n",
      "2 174080 74.140380859375\n",
      "2 176128 73.03900146484375\n",
      "2 178176 72.99415588378906\n",
      "2 180224 72.5650634765625\n",
      "2 182272 73.12158203125\n",
      "2 184320 73.68960571289062\n",
      "2 186368 73.08609771728516\n",
      "2 188416 74.06958770751953\n",
      "2 190464 73.29946899414062\n",
      "2 192512 72.74515533447266\n",
      "2 194560 72.43816375732422\n",
      "2 196608 72.57137298583984\n",
      "2 198656 70.92955017089844\n",
      "2 200704 73.05813598632812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 202752 71.62602996826172\n",
      "2 204800 72.22657012939453\n",
      "2 206848 71.74776458740234\n",
      "2 208896 70.39109802246094\n",
      "2 210944 72.93453979492188\n",
      "F1 Score on dev data: 0.01896\n",
      "3 0 72.57882690429688\n",
      "3 2048 71.08993530273438\n",
      "3 4096 71.66047668457031\n",
      "3 6144 71.21463775634766\n",
      "3 8192 71.3689956665039\n",
      "3 10240 72.88203430175781\n",
      "3 12288 72.42891693115234\n",
      "3 14336 70.98551177978516\n",
      "3 16384 70.75045013427734\n",
      "3 18432 71.65196990966797\n",
      "3 20480 69.95182800292969\n",
      "3 22528 70.98160552978516\n",
      "3 24576 70.85200500488281\n",
      "3 26624 70.00276184082031\n",
      "3 28672 70.70428466796875\n",
      "3 30720 69.74179077148438\n",
      "3 32768 71.02493286132812\n",
      "3 34816 70.06451416015625\n",
      "3 36864 69.51483154296875\n",
      "3 38912 70.10832214355469\n",
      "3 40960 69.49256134033203\n",
      "3 43008 69.57696533203125\n",
      "3 45056 70.63912963867188\n",
      "3 47104 69.62510681152344\n",
      "3 49152 69.93360900878906\n",
      "3 51200 69.13990783691406\n",
      "3 53248 69.66243743896484\n",
      "3 55296 69.40829467773438\n",
      "3 57344 70.31852722167969\n",
      "3 59392 69.70999145507812\n",
      "3 61440 67.93171691894531\n",
      "3 63488 68.24513244628906\n",
      "3 65536 69.85132598876953\n",
      "3 67584 68.61782836914062\n",
      "3 69632 70.1657485961914\n",
      "3 71680 69.81379699707031\n",
      "3 73728 69.37312316894531\n",
      "3 75776 69.12588500976562\n",
      "3 77824 69.01687622070312\n",
      "3 79872 67.86260986328125\n",
      "3 81920 68.98252868652344\n",
      "3 83968 68.64828491210938\n",
      "3 86016 68.7303695678711\n",
      "3 88064 67.59003448486328\n",
      "3 90112 67.40223693847656\n",
      "3 92160 68.12598419189453\n",
      "3 94208 67.26575469970703\n",
      "3 96256 67.74028015136719\n",
      "3 98304 67.75164031982422\n",
      "3 100352 68.33253479003906\n",
      "3 102400 67.50541687011719\n",
      "3 104448 67.40461730957031\n",
      "3 106496 67.00865936279297\n",
      "3 108544 65.77626037597656\n",
      "3 110592 66.66500854492188\n",
      "3 112640 66.97986602783203\n",
      "3 114688 67.57707214355469\n",
      "3 116736 66.75030517578125\n",
      "3 118784 65.86662292480469\n",
      "3 120832 66.90066528320312\n",
      "3 122880 66.78965759277344\n",
      "3 124928 65.54817962646484\n",
      "3 126976 66.21685791015625\n",
      "3 129024 64.68295288085938\n",
      "3 131072 66.06674194335938\n",
      "3 133120 64.69447326660156\n",
      "3 135168 65.83411407470703\n",
      "3 137216 66.44660186767578\n",
      "3 139264 65.22764587402344\n",
      "3 141312 65.14091491699219\n",
      "3 143360 66.71548461914062\n",
      "3 145408 64.77627563476562\n",
      "3 147456 64.97247314453125\n",
      "3 149504 65.50714111328125\n",
      "3 151552 63.933372497558594\n",
      "3 153600 64.74855041503906\n",
      "3 155648 64.97258758544922\n",
      "3 157696 65.3255844116211\n",
      "3 159744 64.121826171875\n",
      "3 161792 63.62538146972656\n",
      "3 163840 64.15129089355469\n",
      "3 165888 64.27604675292969\n",
      "3 167936 63.61670684814453\n",
      "3 169984 64.27073669433594\n",
      "3 172032 64.14631652832031\n",
      "3 174080 64.58609008789062\n",
      "3 176128 63.64653778076172\n",
      "3 178176 63.58038330078125\n",
      "3 180224 63.30485534667969\n",
      "3 182272 63.608665466308594\n",
      "3 184320 64.07431030273438\n",
      "3 186368 63.518245697021484\n",
      "3 188416 64.36540222167969\n",
      "3 190464 63.638221740722656\n",
      "3 192512 63.16057205200195\n",
      "3 194560 62.87633514404297\n",
      "3 196608 62.84965515136719\n",
      "3 198656 61.6190071105957\n",
      "3 200704 63.306480407714844\n",
      "3 202752 62.05159378051758\n",
      "3 204800 62.48739242553711\n",
      "3 206848 62.094627380371094\n",
      "3 208896 60.7962646484375\n",
      "3 210944 62.993675231933594\n",
      "F1 Score on dev data: 0.02045\n",
      "4 0 62.87773132324219\n",
      "4 2048 61.34198760986328\n",
      "4 4096 61.92172622680664\n",
      "4 6144 61.520042419433594\n",
      "4 8192 61.77235412597656\n",
      "4 10240 62.74840545654297\n",
      "4 12288 62.5730094909668\n",
      "4 14336 61.25027847290039\n",
      "4 16384 61.02180480957031\n",
      "4 18432 61.75260543823242\n",
      "4 20480 60.26731872558594\n",
      "4 22528 61.25145721435547\n",
      "4 24576 61.04096603393555\n",
      "4 26624 60.29291534423828\n",
      "4 28672 60.90308380126953\n",
      "4 30720 59.967979431152344\n",
      "4 32768 60.982112884521484\n",
      "4 34816 60.184810638427734\n",
      "4 36864 59.48207473754883\n",
      "4 38912 60.166717529296875\n",
      "4 40960 59.64173126220703\n",
      "4 43008 59.68898010253906\n",
      "4 45056 60.43992614746094\n",
      "4 47104 59.80763626098633\n",
      "4 49152 59.92130661010742\n",
      "4 51200 59.063446044921875\n",
      "4 53248 59.69810485839844\n",
      "4 55296 59.35526657104492\n",
      "4 57344 60.140769958496094\n",
      "4 59392 59.55607986450195\n",
      "4 61440 57.91930389404297\n",
      "4 63488 58.302303314208984\n",
      "4 65536 59.58445358276367\n",
      "4 67584 58.60328674316406\n",
      "4 69632 59.849754333496094\n",
      "4 71680 59.58226013183594\n",
      "4 73728 58.933719635009766\n",
      "4 75776 58.848472595214844\n",
      "4 77824 58.67182159423828\n",
      "4 79872 57.76897048950195\n",
      "4 81920 58.75383758544922\n",
      "4 83968 58.3192138671875\n",
      "4 86016 58.53782272338867\n",
      "4 88064 57.35520935058594\n",
      "4 90112 57.37308120727539\n",
      "4 92160 57.75212860107422\n",
      "4 94208 57.14945983886719\n",
      "4 96256 57.48119354248047\n",
      "4 98304 57.42227554321289\n",
      "4 100352 57.8530158996582\n",
      "4 102400 57.20751190185547\n",
      "4 104448 57.10993957519531\n",
      "4 106496 56.61250305175781\n",
      "4 108544 55.707122802734375\n",
      "4 110592 56.51787567138672\n",
      "4 112640 56.59040832519531\n",
      "4 114688 57.001190185546875\n",
      "4 116736 56.39039611816406\n",
      "4 118784 55.55036544799805\n",
      "4 120832 56.42181396484375\n",
      "4 122880 56.575714111328125\n",
      "4 124928 55.35302734375\n",
      "4 126976 55.831260681152344\n",
      "4 129024 54.555503845214844\n",
      "4 131072 55.574562072753906\n",
      "4 133120 54.60162353515625\n",
      "4 135168 55.44944381713867\n",
      "4 137216 55.83854675292969\n",
      "4 139264 55.06980514526367\n",
      "4 141312 54.77897644042969\n",
      "4 143360 56.103981018066406\n",
      "4 145408 54.455421447753906\n",
      "4 147456 54.76151657104492\n",
      "4 149504 55.06792449951172\n",
      "4 151552 53.71173095703125\n",
      "4 153600 54.51819610595703\n",
      "4 155648 54.569984436035156\n",
      "4 157696 54.85014724731445\n",
      "4 159744 53.89598083496094\n",
      "4 161792 53.38164520263672\n",
      "4 163840 53.83240509033203\n",
      "4 165888 53.78498458862305\n",
      "4 167936 53.46586990356445\n",
      "4 169984 53.82847595214844\n",
      "4 172032 53.79082107543945\n",
      "4 174080 54.18223571777344\n",
      "4 176128 53.31962585449219\n",
      "4 178176 53.31850051879883\n",
      "4 180224 53.08331298828125\n",
      "4 182272 53.2507209777832\n",
      "4 184320 53.63507080078125\n",
      "4 186368 53.21587371826172\n",
      "4 188416 53.89957809448242\n",
      "4 190464 53.160255432128906\n",
      "4 192512 52.824398040771484\n",
      "4 194560 52.50346374511719\n",
      "4 196608 52.52372360229492\n",
      "4 198656 51.64601135253906\n",
      "4 200704 52.939849853515625\n",
      "4 202752 51.83795928955078\n",
      "4 204800 52.23267364501953\n",
      "4 206848 51.96005630493164\n",
      "4 208896 50.87601089477539\n",
      "4 210944 52.628822326660156\n",
      "F1 Score on dev data: 0.02454\n",
      "5 0 52.728843688964844\n",
      "5 2048 51.148040771484375\n",
      "5 4096 51.8563232421875\n",
      "5 6144 51.4495849609375\n",
      "5 8192 51.707763671875\n",
      "5 10240 52.338462829589844\n",
      "5 12288 52.38653564453125\n",
      "5 14336 51.187461853027344\n",
      "5 16384 50.97427749633789\n",
      "5 18432 51.55332565307617\n",
      "5 20480 50.411354064941406\n",
      "5 22528 51.35358428955078\n",
      "5 24576 51.15821838378906\n",
      "5 26624 50.65447998046875\n",
      "5 28672 50.96087646484375\n",
      "5 30720 50.169342041015625\n",
      "5 32768 51.01182556152344\n",
      "5 34816 50.395652770996094\n",
      "5 36864 49.680389404296875\n",
      "5 38912 50.428131103515625\n",
      "5 40960 50.03473663330078\n",
      "5 43008 50.07756042480469\n",
      "5 45056 50.64976119995117\n",
      "5 47104 50.27708053588867\n",
      "5 49152 50.2645378112793\n",
      "5 51200 49.3885498046875\n",
      "5 53248 50.153045654296875\n",
      "5 55296 49.9424934387207\n",
      "5 57344 50.56089782714844\n",
      "5 59392 49.929534912109375\n",
      "5 61440 48.55350112915039\n",
      "5 63488 49.042572021484375\n",
      "5 65536 49.986812591552734\n",
      "5 67584 49.408565521240234\n",
      "5 69632 50.38414764404297\n",
      "5 71680 50.166786193847656\n",
      "5 73728 49.55164337158203\n",
      "5 75776 49.562129974365234\n",
      "5 77824 49.350833892822266\n",
      "5 79872 48.83573913574219\n",
      "5 81920 49.57103729248047\n",
      "5 83968 49.20574188232422\n",
      "5 86016 49.45663070678711\n",
      "5 88064 48.503440856933594\n",
      "5 90112 48.69947814941406\n",
      "5 92160 48.783267974853516\n",
      "5 94208 48.48225784301758\n",
      "5 96256 48.7588005065918\n",
      "5 98304 48.5150146484375\n",
      "5 100352 48.961360931396484\n",
      "5 102400 48.3192138671875\n",
      "5 104448 48.340232849121094\n",
      "5 106496 47.948333740234375\n",
      "5 108544 47.4019775390625\n",
      "5 110592 48.19103240966797\n",
      "5 112640 48.026676177978516\n",
      "5 114688 48.25693130493164\n",
      "5 116736 47.935630798339844\n",
      "5 118784 47.35163879394531\n",
      "5 120832 48.001068115234375\n",
      "5 122880 48.55713653564453\n",
      "5 124928 47.306358337402344\n",
      "5 126976 47.678565979003906\n",
      "5 129024 46.66191101074219\n",
      "5 131072 47.46128845214844\n",
      "5 133120 46.8649787902832\n",
      "5 135168 47.49457550048828\n",
      "5 137216 47.744422912597656\n",
      "5 139264 47.608577728271484\n",
      "5 141312 47.11526107788086\n",
      "5 143360 48.18113327026367\n",
      "5 145408 46.88428497314453\n",
      "5 147456 47.293548583984375\n",
      "5 149504 47.567596435546875\n",
      "5 151552 46.480812072753906\n",
      "5 153600 47.31724548339844\n",
      "5 155648 47.199134826660156\n",
      "5 157696 47.387351989746094\n",
      "5 159744 46.856014251708984\n",
      "5 161792 46.486610412597656\n",
      "5 163840 46.79932403564453\n",
      "5 165888 46.61617660522461\n",
      "5 167936 46.5372314453125\n",
      "5 169984 46.7364387512207\n",
      "5 172032 46.96297836303711\n",
      "5 174080 47.42137908935547\n",
      "5 176128 46.60023498535156\n",
      "5 178176 46.71023941040039\n",
      "5 180224 46.553466796875\n",
      "5 182272 46.71819305419922\n",
      "5 184320 47.083213806152344\n",
      "5 186368 46.90940856933594\n",
      "5 188416 47.525360107421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 190464 46.705055236816406\n",
      "5 192512 46.58042907714844\n",
      "5 194560 46.23357391357422\n",
      "5 196608 46.36640167236328\n",
      "5 198656 45.82695770263672\n",
      "5 200704 46.85900115966797\n",
      "5 202752 45.92211151123047\n",
      "5 204800 46.372703552246094\n",
      "5 206848 46.356903076171875\n",
      "5 208896 45.49235534667969\n",
      "5 210944 46.957176208496094\n",
      "F1 Score on dev data: 0.02911\n",
      "6 0 inf\n",
      "F1 Score on dev data: 0.02911\n"
     ]
    }
   ],
   "source": [
    "if(is_train):\n",
    "    dev_target, dev_input = get_batch_data(train_size, data_size)\n",
    "    train(net, optimizer, reg_lambda, max_epochs, dev_input, dev_target, batch_size, train_size)\n",
    "\n",
    "    torch.save(net.weights, WEIGHTS_PATH + 'cbow_weights_' + str(vocab_size) + '.pt')\n",
    "    torch.save(net.biases, WEIGHTS_PATH + 'cbow_biases_' + str(vocab_size) + '.pt')\n",
    "else:\n",
    "    net.weights = torch.load(WEIGHTS_PATH + 'cbow_weights_' + str(vocab_size) + '.pt')\n",
    "    net.biases = torch.load(WEIGHTS_PATH + 'cbow_biases_' + str(vocab_size) + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6840339",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "01659421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 991/991 [00:05<00:00, 171.00it/s]\n",
      "100%|█████████████████████████████████████| 991/991 [00:00<00:00, 126385.16it/s]\n"
     ]
    }
   ],
   "source": [
    "validation = pd.read_csv(VALIDATION, sep=' ', names=['word1','word2','word3','word4'])\n",
    "\n",
    "validation['word1'] = validation['word1'].apply(lambda x : x.lower())\n",
    "validation['word2'] = validation['word2'].apply(lambda x : x.lower())\n",
    "validation['word3'] = validation['word3'].apply(lambda x : x.lower())\n",
    "validation['word4'] = validation['word4'].apply(lambda x : x.lower())\n",
    "\n",
    "validation['pred'] = validation[['word1','word2','word3']].progress_apply(lambda x : get_result(x['word1'], x['word2'], x['word3'], problem='cbow'), axis = 1)\n",
    "validation['found'] = validation[['word4','pred']].progress_apply(lambda x : x['word4'] in x['pred'], axis=1)\n",
    "\n",
    "count = len(os.listdir(RESULTS_PATH))\n",
    "validation.to_csv(RESULTS_PATH + \"value_\"+str(count)+'_'+str(vocab_size)+'_'+str(no_of_rows)+problem+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cf94f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>pred</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word1, word2, word3, word4, pred, found]\n",
       "Index: []"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[validation['pred']==validation['word4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4fb20a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>pred</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>see</td>\n",
       "      <td>sees</td>\n",
       "      <td>became</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>shuffle</td>\n",
       "      <td>shuffles</td>\n",
       "      <td>became</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sing</td>\n",
       "      <td>sings</td>\n",
       "      <td>august</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sit</td>\n",
       "      <td>sits</td>\n",
       "      <td>august</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>slow</td>\n",
       "      <td>slows</td>\n",
       "      <td>august</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>naira</td>\n",
       "      <td>archived</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>population</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>japan</td>\n",
       "      <td>yen</td>\n",
       "      <td>archived</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>retrieved</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>denmark</td>\n",
       "      <td>krone</td>\n",
       "      <td>times</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1  word2    word3     word4        pred  found\n",
       "0         walk  walks      see      sees      became  False\n",
       "1         walk  walks  shuffle  shuffles      became  False\n",
       "2         walk  walks     sing     sings      august  False\n",
       "3         walk  walks      sit      sits      august  False\n",
       "4         walk  walks     slow     slows      august  False\n",
       "..         ...    ...      ...       ...         ...    ...\n",
       "986  argentina   peso  nigeria     naira    archived  False\n",
       "987  argentina   peso     iran      rial  population  False\n",
       "988  argentina   peso    japan       yen    archived  False\n",
       "989      india  rupee     iran      rial   retrieved  False\n",
       "990      india  rupee  denmark     krone       times  False\n",
       "\n",
       "[991 rows x 6 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26aee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02e04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "076b7bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'government'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result('jan','january','dec' , problem='cbow')\n",
    "\n",
    "# get_similarity('queen', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe5019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9df9cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 991/991 [00:04<00:00, 199.44it/s]\n",
      "100%|█████████████████████████████████████| 991/991 [00:00<00:00, 207102.90it/s]\n"
     ]
    }
   ],
   "source": [
    "validation = pd.read_csv(VALIDATION, sep=' ', names=['word1','word2','word3','word4'])\n",
    "\n",
    "validation['word1'] = validation['word1'].apply(lambda x : x.lower())\n",
    "validation['word2'] = validation['word2'].apply(lambda x : x.lower())\n",
    "validation['word3'] = validation['word3'].apply(lambda x : x.lower())\n",
    "validation['word4'] = validation['word4'].apply(lambda x : x.lower())\n",
    "\n",
    "validation['pred'] = validation[['word1','word2','word3']].progress_apply(lambda x : get_result(x['word1'], x['word2'], x['word3'], problem='skipgram'), axis = 1)\n",
    "validation['found'] = validation[['word4','pred']].progress_apply(lambda x : x['word4'] in x['pred'], axis=1)\n",
    "\n",
    "count = len(os.listdir(RESULTS_PATH))\n",
    "validation.to_csv(RESULTS_PATH + \"value_\"+str(count)+'_'+str(vocab_size)+'_'+str(no_of_rows)+problem+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98ab868c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>pred</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word1, word2, word3, word4, pred, found]\n",
       "Index: []"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[validation['found']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76efc7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>pred</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>see</td>\n",
       "      <td>sees</td>\n",
       "      <td>[india, largest, city, system]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>shuffle</td>\n",
       "      <td>shuffles</td>\n",
       "      <td>[largest, city, one, budapest]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sing</td>\n",
       "      <td>sings</td>\n",
       "      <td>[largest, budapest, city, portugal]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sit</td>\n",
       "      <td>sits</td>\n",
       "      <td>[largest, states, city, europe]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>slow</td>\n",
       "      <td>slows</td>\n",
       "      <td>[largest, many, also, country]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>naira</td>\n",
       "      <td>[among, india, world, state]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>[city, india, world, national]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>japan</td>\n",
       "      <td>yen</td>\n",
       "      <td>[national, north, world, india]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>[city, new, one, area]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>denmark</td>\n",
       "      <td>krone</td>\n",
       "      <td>[area, worlds, city, august]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1  word2    word3     word4                                 pred  \\\n",
       "0         walk  walks      see      sees       [india, largest, city, system]   \n",
       "1         walk  walks  shuffle  shuffles       [largest, city, one, budapest]   \n",
       "2         walk  walks     sing     sings  [largest, budapest, city, portugal]   \n",
       "3         walk  walks      sit      sits      [largest, states, city, europe]   \n",
       "4         walk  walks     slow     slows       [largest, many, also, country]   \n",
       "..         ...    ...      ...       ...                                  ...   \n",
       "986  argentina   peso  nigeria     naira         [among, india, world, state]   \n",
       "987  argentina   peso     iran      rial       [city, india, world, national]   \n",
       "988  argentina   peso    japan       yen      [national, north, world, india]   \n",
       "989      india  rupee     iran      rial               [city, new, one, area]   \n",
       "990      india  rupee  denmark     krone         [area, worlds, city, august]   \n",
       "\n",
       "     found  \n",
       "0    False  \n",
       "1    False  \n",
       "2    False  \n",
       "3    False  \n",
       "4    False  \n",
       "..     ...  \n",
       "986  False  \n",
       "987  False  \n",
       "988  False  \n",
       "989  False  \n",
       "990  False  \n",
       "\n",
       "[991 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28e9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
