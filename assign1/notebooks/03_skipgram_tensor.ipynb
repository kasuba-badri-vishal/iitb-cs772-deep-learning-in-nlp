{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afa00d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "text_counter = Counter()\n",
    "tqdm.pandas()\n",
    "mps_device = torch.device(\"mps\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aab0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = './../weights/'\n",
    "DATASET_PATH = './../data/processed/processed_data.csv'\n",
    "INTERMEDIATE_PATH = './../intermediate/'\n",
    "RESULTS_PATH = './../results/'\n",
    "VALIDATION = './../data/Validation.txt'\n",
    "\n",
    "embedding_size = 512\n",
    "no_of_rows     = 20000\n",
    "window_size    = 3\n",
    "num_neg_sam    = 0\n",
    "max_epochs     = 25\n",
    "learning_rate  = 0.001\n",
    "reg_lambda     = 1\n",
    "batch_size     = 2048\n",
    "train_split    = 0.8\n",
    "is_train       = True\n",
    "problem        = \"skipgram\"\n",
    "new_weights    = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12914f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.y_pred = None\n",
    "        self.emb    = None\n",
    "\n",
    "        if(os.path.exists('./weights/' + 'skipgram_weights_' + str(vocab_size) + '.pt') and new_weights==False):\n",
    "            self.weights = torch.load(WEIGHTS_PATH + 'skipgram_weights_' + str(vocab_size) + '.pt')\n",
    "            self.biases = torch.load(WEIGHTS_PATH + 'skipgram_biases_' + str(vocab_size) + '.pt')\n",
    "        else:\n",
    "            self.weights = []\n",
    "            self.biases  = []\n",
    "            \n",
    "            self.weights.append(torch.rand((vocab_size, embedding_size), device=mps_device) * 2 - 1)\n",
    "            self.weights.append(torch.rand((embedding_size, vocab_size), device=mps_device) * 2 - 1)\n",
    "            self.biases.append(torch.rand((embedding_size), device=mps_device) * 2 - 1)\n",
    "            self.biases.append(torch.rand((vocab_size), device=mps_device) * 2 - 1)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        self.emb = (torch.matmul(X,self.weights[0]) + self.biases[0])\n",
    "        self.y_pred = torch.softmax(torch.matmul(self.emb, self.weights[1]) + self.biases[1], dim=1)\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, X, y, lamda):\n",
    "        \n",
    "        del_W = []\n",
    "        del_b = []\n",
    "\n",
    "        delta = self.y_pred - y\n",
    "        del_b.insert(0,torch.sum(delta, axis = 0, keepdims = True))\n",
    "        del_W.insert(0,torch.matmul(self.emb.T, delta) + lamda * (self.weights[1]))\n",
    "\n",
    "        delta = torch.matmul(delta, self.weights[1].T) * (self.emb)\n",
    "        del_b.insert(0,torch.sum(delta, axis = 0, keepdims = True))\n",
    "        del_W.insert(0,torch.matmul(X.T, delta) + lamda * (self.weights[0]))\n",
    "        return del_W, del_b\n",
    "    \n",
    "class Optimizer(object):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, learning_rate, weights, biases, optimizer=\"gradient\"):\n",
    "        \n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.m_dw = [torch.zeros((w.shape), device=mps_device) for w in weights]\n",
    "        self.m_db = [torch.zeros((b.shape), device=mps_device) for b in biases]\n",
    "        self.v_dw = [torch.zeros((w.shape), device=mps_device) for w in weights]\n",
    "        self.v_db = [torch.zeros((b.shape), device=mps_device) for b in biases]\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.eta = learning_rate\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, weights, biases, delta_weights, delta_biases):\n",
    "        \n",
    "        if(self.optimizer == \"gradient\"):\n",
    "            return self.gradient(weights, biases, delta_weights, delta_biases)\n",
    "        elif(self.optimizer == \"adam\"):\n",
    "            return self.adam(weights, biases, delta_weights, delta_biases)\n",
    "    \n",
    "    def adam(self, weights, biases, delta_weights, delta_biases):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_dw = [self.beta1 * m + (1 - self.beta1) * del_w for m, del_w in zip(self.m_dw, delta_weights)]\n",
    "        self.m_db = [self.beta1 * m + (1 - self.beta1) * del_b for m, del_b in zip(self.m_db, delta_biases)]\n",
    "        self.v_dw = [self.beta2 * v + (1 - self.beta2) * (del_w**2) for v, del_w in zip(self.v_dw, delta_weights)]\n",
    "        self.v_db = [self.beta2 * v + (1 - self.beta2) * (del_b**2) for v, del_b in zip(self.v_db, delta_biases)]\n",
    "\n",
    "        # bias correction\n",
    "        m_hat_dw = [m / (1 - self.beta1 ** self.t) for m in self.m_dw]\n",
    "        v_hat_dw = [v / (1 - self.beta2 ** self.t) for v in self.v_dw]\n",
    "\n",
    "        m_hat_db = [m / (1 - self.beta1 ** self.t) for m in self.m_db]\n",
    "        v_hat_db = [v / (1 - self.beta2 ** self.t) for v in self.v_db]\n",
    "\n",
    "        # update weights and biases\n",
    "        weights = [w - self.eta * m_hat / ((torch.sqrt(v_hat) + self.epsilon)) for w, m_hat, v_hat in zip(weights, m_hat_dw, v_hat_dw)] \n",
    "        biases = [b - self.eta * m_hat / ((torch.sqrt(v_hat) + self.epsilon)) for b, m_hat, v_hat in zip(biases, m_hat_db, v_hat_db)]\n",
    "        return weights, biases\n",
    "\n",
    "    \n",
    "    def gradient(self, weights, biases, delta_weights, delta_biases):\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i] - self.eta*delta_weights[i]\n",
    "            biases[i] = biases[i] - self.eta*delta_biases[i]\n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56270eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word):\n",
    "    try:\n",
    "        val = word2idx[word]\n",
    "        input_arr = torch.zeros((vocab_size), dtype=torch.float, device=mps_device)\n",
    "        input_arr[val] = 1\n",
    "        emb = torch.matmul(input_arr,net.weights[0]) + net.biases[0]\n",
    "        return emb\n",
    "    except:\n",
    "        return torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "\n",
    "def get_result(word1, word2, word3, problem='skipgram', window=4):\n",
    "    w1_emb = get_embeddings(word1)\n",
    "    w2_emb = get_embeddings(word2)    \n",
    "    w3_emb = get_embeddings(word3)    \n",
    "    if((torch.numel(w1_emb) != 0) and (torch.numel(w2_emb) != 0) and (torch.numel(w3_emb) != 0)):\n",
    "        w4_emb = w1_emb + w3_emb - w2_emb\n",
    "        output = torch.softmax(torch.matmul(w4_emb, net.weights[1]) + net.biases[1], dim=1)\n",
    "        if(problem=='skipgram'):\n",
    "            topk_preds = torch.topk(output, k=window).indices.tolist()\n",
    "            ans = [idx2word[x] for x in topk_preds[0]]\n",
    "        else:\n",
    "            ans = idx2word[torch.argmax(output).item()]\n",
    "        return ans\n",
    "    return \"UNK\"\n",
    "\n",
    "\n",
    "def get_embeddings(word):\n",
    "    try:\n",
    "        val = word2idx[word]\n",
    "        input_arr = torch.zeros((vocab_size), dtype=torch.float, device=mps_device)\n",
    "        input_arr[val] = 1\n",
    "        emb = torch.matmul(input_arr,net.weights[0]) + net.biases[0]\n",
    "        return emb\n",
    "    except:\n",
    "        return torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "    \n",
    "def get_accuracy(validation):\n",
    "    accuracy = torch.empty(0, dtype=torch.float, device=mps_device)\n",
    "    for _, row in validation.iterrows():\n",
    "        w1_emb = get_embeddings(row['word1'])\n",
    "        w2_emb = get_embeddings(row['word2'])    \n",
    "        w3_emb = get_embeddings(row['word3'])\n",
    "        w4_emb = get_embeddings(row['word4'])\n",
    "        \n",
    "        if((torch.numel(w1_emb) != 0) and (torch.numel(w2_emb) != 0) and (torch.numel(w3_emb) != 0)):\n",
    "            pred_emb = w1_emb + w3_emb - w2_emb\n",
    "            accuracy = torch.cat(accuracy, cosine_similarity(pred_emb, w4_emb), dim=0)\n",
    "    return torch.mean(accuracy, dim=0)\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    C = torch.squeeze(A)\n",
    "    D = torch.squeeze(B)\n",
    "    return torch.dot(C, D) / (torch.norm(C) * torch.norm(D))\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    if(mps_device.type == 'mps'):\n",
    "        return -torch.mean(custom_nansum(y_true * torch.log(y_pred), axis=-1))\n",
    "    else:\n",
    "        return -torch.mean(torch.nansum(y_true * torch.log(y_pred), axis=-1))\n",
    "\n",
    "def get_dataset(words, window_size):\n",
    "    data = pd.DataFrame(columns=[\"word\", \"context_words\"])\n",
    "    for index, word in enumerate(words):\n",
    "        context_words = words[max(0, index - window_size): index] + words[index + 1: index + window_size + 1]\n",
    "        context_words = list(set(context_words))\n",
    "        data.loc[len(data.index)] = [word, context_words]\n",
    "    global dataset\n",
    "    dataset = pd.concat([dataset,data])\n",
    "    \n",
    "def custom_nansum(input_tensor, axis=None):\n",
    "    # Replace NaN values with 0\n",
    "    input_tensor = torch.where(torch.isnan(input_tensor), torch.tensor(0., device=input_tensor.device), input_tensor)\n",
    "    \n",
    "    # Compute the sum over the specified axis\n",
    "    if axis is None:\n",
    "        return torch.sum(input_tensor)\n",
    "    else:\n",
    "        return torch.sum(input_tensor, dim=axis)\n",
    "    \n",
    "    \n",
    "def get_batch_data(start_index, end_index):\n",
    "\n",
    "    batch_dataset = dataset[start_index:end_index].reset_index(drop=True)\n",
    "    batch_size = batch_dataset.shape[0]\n",
    "    batch_input = torch.zeros([batch_size, vocab_size], dtype=torch.float, device=mps_device)\n",
    "    batch_output = torch.zeros([batch_size, vocab_size], dtype=torch.float, device=mps_device)\n",
    "\n",
    "    for index, data in batch_dataset.iterrows():\n",
    "        batch_input[index, data['word']] = 1\n",
    "        for ind in data['context_words']:\n",
    "            batch_output[index, ind] = 1\n",
    "            \n",
    "    return batch_input, batch_output\n",
    "\n",
    "\n",
    "def train(net, optimizer, lamda, max_epochs, dev_input, dev_target, batch_size, train_size):\n",
    "\n",
    "    stop = False\n",
    "    value = 999999999\n",
    "\n",
    "    for e in range(max_epochs):\n",
    "        epoch_loss = 0\n",
    "        first_loss = True\n",
    "        for start_index in range(0, train_size, batch_size):\n",
    "            end_index = min(start_index + batch_size, train_size)\n",
    "            batch_input, batch_target = get_batch_data(start_index, end_index)\n",
    "            pred = net(batch_input)\n",
    "\n",
    "            # Compute gradients of loss w.r.t. weights and biases\n",
    "            dW, db = net.backward(batch_input, batch_target, lamda)\n",
    "\n",
    "            # Get updated weights based on current weights and gradients\n",
    "            weights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)\n",
    "\n",
    "            # Update model's weights and biases\n",
    "            net.weights = weights_updated\n",
    "            net.biases = biases_updated\n",
    "            loss = cross_entropy_loss(pred, batch_target)\n",
    "            print(e, start_index, loss.item())\n",
    "            if(torch.isnan(loss)):\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "            if(first_loss):\n",
    "                first_loss = False\n",
    "                if(value<loss.item()):\n",
    "                    stop = True\n",
    "                    break\n",
    "                else:\n",
    "                    value = loss.item()\n",
    "            \n",
    "\n",
    "        dev_pred = net(dev_input)\n",
    "        indices = torch.topk(dev_pred, k=window_size, dim=1)[1][:, -window_size:]\n",
    "        converted_matrix = torch.zeros_like(dev_pred)\n",
    "        converted_matrix[torch.arange(dev_pred.shape[0])[:, None], indices] = 1\n",
    "        numpy_dev_target = dev_target.cpu().numpy()\n",
    "        converted_matrix = converted_matrix.cpu().numpy()\n",
    "        print('F1 Score on dev data: {:.5f}'.format(f1_score(numpy_dev_target, converted_matrix, average='micro')))\n",
    "\n",
    "        if(stop):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec17f0",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f657279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH ,nrows = no_of_rows)\n",
    "df = df.dropna()\n",
    "\n",
    "df['sentences'] = df['sentences'].apply(lambda sentence : sentence.split())\n",
    "_ = df['sentences'].apply(text_counter.update)\n",
    "text_counter.update(['UNK'])\n",
    "vocab_size = len(text_counter)\n",
    "\n",
    "words, _ = zip(*text_counter.most_common(vocab_size))\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "idx2word = {i: w for i, w in enumerate(words)}\n",
    "\n",
    "\n",
    "# dataset = pd.read_pickle('dataset.pkl')\n",
    "df['sentences'] = df['sentences'].apply(lambda words : [word2idx[word] for word in words])\n",
    "\n",
    "if(no_of_rows==None and os.path.exists(INTERMEDIATE_PATH + 'dataset.pkl')):\n",
    "   dataset = pd.read_pickle(INTERMEDIATE_PATH + 'dataset.pkl') \n",
    "\n",
    "elif(os.path.exists(INTERMEDIATE_PATH + 'dataset_'+str(no_of_rows)+'.pkl')):\n",
    "    dataset = pd.read_pickle(INTERMEDIATE_PATH + 'dataset_'+str(no_of_rows)+'.pkl')\n",
    "else:\n",
    "    dataset = pd.DataFrame(columns=[\"word\", \"context_words\"])\n",
    "    _ = df['sentences'].progress_apply(lambda x : get_dataset(x, window_size))\n",
    "\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    if(no_of_rows==None):\n",
    "        dataset.to_pickle(INTERMEDIATE_PATH + 'dataset.pkl') \n",
    "    else:\n",
    "        dataset.to_pickle(INTERMEDIATE_PATH + 'dataset_'+str(no_of_rows)+'.pkl')\n",
    "\n",
    "data_size = dataset.shape[0]\n",
    "\n",
    "print(\"Vocab length :\", vocab_size)\n",
    "print(\"Dataset size :\", data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a02f8",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "061fde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = max(int(0.90*data_size), data_size-5000)\n",
    "\n",
    "net = Net()\n",
    "optimizer = Optimizer(learning_rate, net.weights, net.biases, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88fbf234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 inf\n",
      "F1 Score on dev data: 0.00022\n"
     ]
    }
   ],
   "source": [
    "if(is_train):\n",
    "    dev_input, dev_target = get_batch_data(train_size, data_size)\n",
    "    train(net, optimizer, reg_lambda, max_epochs, dev_input, dev_target, batch_size, train_size)\n",
    "\n",
    "    torch.save(net.weights, WEIGHTS_PATH + 'skipgram_weights_' + str(vocab_size) + '.pt')\n",
    "    torch.save(net.biases, WEIGHTS_PATH + 'skipgram_biases_' + str(vocab_size) + '.pt')\n",
    "else:\n",
    "    net.weights = torch.load(WEIGHTS_PATH + 'skipgram_weights_' + str(vocab_size) + '.pt')\n",
    "    net.biases = torch.load(WEIGHTS_PATH + 'skipgram_biases_' + str(vocab_size) + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6840339",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01659421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 991/991 [00:02<00:00, 387.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 991/991 [00:00<00:00, 84326.86it/s]\n"
     ]
    }
   ],
   "source": [
    "validation = pd.read_csv(VALIDATION, sep=' ', names=['word1','word2','word3','word4'])\n",
    "\n",
    "validation['word1'] = validation['word1'].apply(lambda x : x.lower())\n",
    "validation['word2'] = validation['word2'].apply(lambda x : x.lower())\n",
    "validation['word3'] = validation['word3'].apply(lambda x : x.lower())\n",
    "validation['word4'] = validation['word4'].apply(lambda x : x.lower())\n",
    "\n",
    "validation['pred'] = validation[['word1','word2','word3']].progress_apply(lambda x : get_result(x['word1'], x['word2'], x['word3'], problem=problem), axis = 1)\n",
    "validation['found'] = validation[['word4','pred']].progress_apply(lambda x : x['word4'] in x['pred'], axis=1)\n",
    "\n",
    "# count = len(os.listdir(RESULTS_PATH))\n",
    "# validation.to_csv(RESULTS_PATH + \"value_\"+str(count)+'_'+str(vocab_size)+'_'+str(no_of_rows)+problem+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a76cfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>pred</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word1, word2, word3, word4, pred, found]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[validation['found']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a294f8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>pred</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>see</td>\n",
       "      <td>sees</td>\n",
       "      <td>[noreg, ur, welovebudapestcom, route]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>shuffle</td>\n",
       "      <td>shuffles</td>\n",
       "      <td>[predominates, hasmat, char, hermit]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sing</td>\n",
       "      <td>sings</td>\n",
       "      <td>[shiites, requested, predominates, opinion]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>sit</td>\n",
       "      <td>sits</td>\n",
       "      <td>[prevented, turkic, corrupted, missouri]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "      <td>slow</td>\n",
       "      <td>slows</td>\n",
       "      <td>[sanskritised, iberoamerican, collars, ur]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>naira</td>\n",
       "      <td>[haidar, ottomans, ca, battery]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>[fainted, norsemen, garment, anna]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>argentina</td>\n",
       "      <td>peso</td>\n",
       "      <td>japan</td>\n",
       "      <td>yen</td>\n",
       "      <td>[tricycles, sponge, shahdo, ensure]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>iran</td>\n",
       "      <td>rial</td>\n",
       "      <td>[distinguished, colonizing, ahmednagar, consta...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>india</td>\n",
       "      <td>rupee</td>\n",
       "      <td>denmark</td>\n",
       "      <td>krone</td>\n",
       "      <td>[kardamaka, snake, increased, servant]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1  word2    word3     word4  \\\n",
       "0         walk  walks      see      sees   \n",
       "1         walk  walks  shuffle  shuffles   \n",
       "2         walk  walks     sing     sings   \n",
       "3         walk  walks      sit      sits   \n",
       "4         walk  walks     slow     slows   \n",
       "..         ...    ...      ...       ...   \n",
       "986  argentina   peso  nigeria     naira   \n",
       "987  argentina   peso     iran      rial   \n",
       "988  argentina   peso    japan       yen   \n",
       "989      india  rupee     iran      rial   \n",
       "990      india  rupee  denmark     krone   \n",
       "\n",
       "                                                  pred  found  \n",
       "0                [noreg, ur, welovebudapestcom, route]  False  \n",
       "1                 [predominates, hasmat, char, hermit]  False  \n",
       "2          [shiites, requested, predominates, opinion]  False  \n",
       "3             [prevented, turkic, corrupted, missouri]  False  \n",
       "4           [sanskritised, iberoamerican, collars, ur]  False  \n",
       "..                                                 ...    ...  \n",
       "986                    [haidar, ottomans, ca, battery]  False  \n",
       "987                 [fainted, norsemen, garment, anna]  False  \n",
       "988                [tricycles, sponge, shahdo, ensure]  False  \n",
       "989  [distinguished, colonizing, ahmednagar, consta...  False  \n",
       "990             [kardamaka, snake, increased, servant]  False  \n",
       "\n",
       "[991 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7728c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
