{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8186d74",
   "metadata": {},
   "source": [
    "# <font color='purple'>Web Scraping</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d484d2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbaa8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "from nltk import sent_tokenize, tokenize\n",
    "from nltk.corpus import wordnet, gutenberg\n",
    "\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79668117",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_SENTENCES = 400\n",
    "\n",
    "ANOLOGY_DATASET = './../data/Analogy_dataset.txt'\n",
    "VALIDATION_DATASET = './../data/Validation.txt'\n",
    "OUTPUT_DIR = './../data/pickle/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7b39e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26610638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data_from_tables(url, col_size=3, concatenate_string='', name=None):\n",
    "    sentences = []\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    table = BeautifulSoup(html, \"html.parser\").find('table')\n",
    "\n",
    "    for row in tqdm(table.find_all('tr')):\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) == col_size:\n",
    "            if(name=='tenses'):\n",
    "                sentences.append('Present: ' + cols[0].text.strip() + ' Past: ' +  cols[1].text.strip() + ' Past participle: ' +  cols[2].text.strip())\n",
    "            else:\n",
    "                sentences.append(cols[1].text.strip() + concatenate_string + cols[0].text.strip())    \n",
    "    \n",
    "    return sentences\n",
    "            \n",
    "\n",
    "def get_top_n_sentences(word1, word2, tokens, length_threshold=200):\n",
    "    sentences = []\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    for sentence in tokens:\n",
    "        is_word1 = (word1 in sentence)\n",
    "        is_word2 = (word2 in sentence)\n",
    "        if(len(sentence)<length_threshold):\n",
    "            if(is_word1 and is_word2):\n",
    "                sentences.insert(0,sentence)\n",
    "                count +=1\n",
    "            elif(is_word1):\n",
    "                sentences.insert(count, sentence)\n",
    "                count2 += 1\n",
    "            elif(is_word2):\n",
    "                sentences.insert(count+count2,sentence)\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "    if(len(sentences)>NO_OF_SENTENCES):\n",
    "        return sentences[:NO_OF_SENTENCES]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def fetch_states_information(url):\n",
    "    sentences = []\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    tables = BeautifulSoup(html, \"html.parser\").find_all('table',{'class':'wikitable sortable plainrowheaders'})\n",
    "\n",
    "    sentences = []\n",
    "    for table in tqdm(tables):\n",
    "        for row in table.find_all('tr'):\n",
    "            value = row.find_all('th')\n",
    "            if(len(value)==1):\n",
    "                name = value[0].text.strip()\n",
    "            cols = row.find_all('td')\n",
    "            length = len(cols)\n",
    "            if(length>7):\n",
    "                sentences.append(name + ' capital is ' + cols[3].text.strip())\n",
    "                sentences.append(name + ' zone is ' + cols[2].text.strip())\n",
    "                sentences.append(name + ' abbreviation is ' + cols[1].text.strip())\n",
    "                sentences.append(name + ' langauge is ' + cols[length-2].text.strip())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def get_sentences(word1, word2):\n",
    "    try:\n",
    "        summary = wikipedia.summary(word1)\n",
    "        sentences = tokenize.sent_tokenize(summary.lower())\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        word = random.choice(e.options)\n",
    "        try:\n",
    "            summary = wikipedia.summary(word)\n",
    "            sentences = tokenize.sent_tokenize(summary.lower())\n",
    "        except:\n",
    "            sentences = []\n",
    "    except:\n",
    "        sentences = []\n",
    "\n",
    "    return get_top_n_sentences(word1, word2, sentences)\n",
    "\n",
    "\n",
    "def get_wiki_sentences(word1, word2):\n",
    "    sentences = []\n",
    "    url = 'https://en.wikipedia.org/wiki/' + word1\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    table = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    raw = BeautifulSoup.get_text(table)\n",
    "    raw = raw.replace(\"\\n\", \"\")\n",
    "    raw = raw.lower()\n",
    "\n",
    "    sentences = sent_tokenize(raw)\n",
    "    return get_top_n_sentences(word1, word2, sentences)\n",
    "\n",
    "\n",
    "def get_wordnet_sentences(word):\n",
    "    sentences = []\n",
    "    syns = wordnet.synsets(word)\n",
    "    for val in syns:\n",
    "        sentences += val.examples()\n",
    "        \n",
    "    ## If no examples found, get synonyms and antonyms from wordnet\n",
    "    if(len(sentences)<3):\n",
    "        result = ''\n",
    "        for val in syns:\n",
    "            for l in val.lemmas():\n",
    "                result += l.name() + ' '\n",
    "                if l.antonyms():\n",
    "                    result += l.antonyms()[0].name() + ' '\n",
    "        sentences.append(result)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025232d",
   "metadata": {},
   "source": [
    "## <font color='dark-orange'>Web Scraping from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21b730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_capitals_url = 'https://geographyfieldwork.com/WorldCapitalCities.htm'\n",
    "currencies_url      = 'https://www.jagranjosh.com/general-knowledge/list-of-countries-and-currencies-of-the-world-1662462803-1'\n",
    "nation_states_url   = 'https://en.wikipedia.org/wiki/States_and_union_territories_of_India'\n",
    "tenses_url          = 'https://kpu.pressbooks.pub/effectiveenglish/chapter/__unknown__/'\n",
    "single_plural_url   = 'https://www.englishbix.com/singular-and-plural-verbs-examples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e59e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 205/205 [00:00<00:00, 35658.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:00<00:00, 19645.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 26317.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 49167.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1218.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped sentences :716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "sentences += scrape_data_from_tables(nation_capitals_url, col_size=2, concatenate_string=' capital is ')\n",
    "sentences += scrape_data_from_tables(currencies_url, col_size=2, concatenate_string=' currency is ')\n",
    "sentences += scrape_data_from_tables(tenses_url, col_size=3, concatenate_string='', name='tenses')\n",
    "sentences += scrape_data_from_tables(tenses_url, col_size=3, concatenate_string='', name='plural')\n",
    "sentences += fetch_states_information(nation_states_url)\n",
    "print(\"Scraped sentences :\" + str(len(sentences)))\n",
    "\n",
    "### Saving Results\n",
    "with open(OUTPUT_DIR + 'url_sentences_v0.pickle', 'wb') as file:\n",
    "    pickle.dump(sentences, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1f339",
   "metadata": {},
   "source": [
    "## <font color='dark-orange'>Wikipedia Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c79c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [03:32<00:00,  2.31s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [03:50<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped sentences :2039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Get Information from wikipedia library\n",
    "wiki_sentences = []\n",
    "\n",
    "df = pd.read_csv(ANOLOGY_DATASET, sep=' ', names=['w1','w2','w3','w4'])\n",
    "df2 = df[['w3','w4']].rename(columns={'w3':'w1', 'w4':'w2'})\n",
    "df = pd.concat([df[['w1','w2']],df2], ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['w1'] = df['w1'].apply(lambda x : x.lower())\n",
    "df['w2'] = df['w2'].apply(lambda x : x.lower())\n",
    "\n",
    "df['sentences1'] = df[['w1','w2']].progress_apply(lambda x : get_sentences(x['w1'],x['w2']), axis=1)\n",
    "df['sentences2'] = df[['w1','w2']].progress_apply(lambda x : get_sentences(x['w2'],x['w1']), axis=1)\n",
    "\n",
    "wiki_sentences += df.sentences1.sum()\n",
    "wiki_sentences += df.sentences2.sum()\n",
    "\n",
    "print(\"Scraped sentences :\" + str(len(wiki_sentences)))\n",
    "\n",
    "\n",
    "### Saving the Results\n",
    "with open(OUTPUT_DIR + 'wiki_sentences.pickle', 'wb') as file:\n",
    "    pickle.dump(wiki_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb5170",
   "metadata": {},
   "source": [
    "## <font color='dark-orange'>Wordnet Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b1570c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:01<00:00, 54.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:00<00:00, 9764.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped sentences :283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(ANOLOGY_DATASET, sep=' ', names=['w1','w2','w3','w4'])\n",
    "\n",
    "df2 = df[['w3','w4']].rename(columns={'w3':'w1', 'w4':'w2'})\n",
    "df = pd.concat([df[['w1','w2']],df2], ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['w1'] = df['w1'].apply(lambda x : x.lower())\n",
    "df['w2'] = df['w2'].apply(lambda x : x.lower())\n",
    "\n",
    "df['sentences1'] = df['w1'].progress_apply(lambda x : get_wordnet_sentences(x))\n",
    "df['sentences2'] = df['w2'].progress_apply(lambda x : get_wordnet_sentences(x))\n",
    "\n",
    "wordnet_sentences= []\n",
    "wordnet_sentences += df.sentences1.sum()\n",
    "wordnet_sentences += df.sentences2.sum()\n",
    "print(\"Scraped sentences :\" + str(len(wordnet_sentences)))\n",
    "\n",
    "### Saving the Results\n",
    "with open(OUTPUT_DIR + 'wordnet_sentences.pickle', 'wb') as file:\n",
    "    pickle.dump(wordnet_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a816b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 326/326 [00:00<00:00, 2331.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 326/326 [00:00<00:00, 20595.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped sentences :9930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(VALIDATION_DATASET, sep=' ', names=['w1','w2','w3','w4'])\n",
    "\n",
    "df2 = df[['w3','w4']].rename(columns={'w3':'w1', 'w4':'w2'})\n",
    "df = pd.concat([df[['w1','w2']],df2], ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['w1'] = df['w1'].apply(lambda x : x.lower())\n",
    "df['w2'] = df['w2'].apply(lambda x : x.lower())\n",
    "\n",
    "df['sentences1'] = df['w1'].progress_apply(lambda x : get_wordnet_sentences(x))\n",
    "df['sentences2'] = df['w2'].progress_apply(lambda x : get_wordnet_sentences(x))\n",
    "\n",
    "wordnet_sentences= []\n",
    "wordnet_sentences += df.sentences1.sum()\n",
    "wordnet_sentences += df.sentences2.sum()\n",
    "print(\"Scraped sentences :\" + str(len(wordnet_sentences)))\n",
    "\n",
    "### Saving the Results\n",
    "with open(OUTPUT_DIR + 'wordnet_sentences2.pickle', 'wb') as file:\n",
    "    pickle.dump(wordnet_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea0eb5",
   "metadata": {},
   "source": [
    "## <font color='dark-orange'>Large scale sentence fetch from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d19b4e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [02:47<00:00,  1.82s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [01:51<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped sentences :57822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_sentences = []\n",
    "\n",
    "df = pd.read_csv(ANOLOGY_DATASET, sep=' ', names=['w1','w2','w3','w4'])\n",
    "df2 = df[['w3','w4']].rename(columns={'w3':'w1', 'w4':'w2'})\n",
    "df = pd.concat([df[['w1','w2']],df2], ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['w1'] = df['w1'].apply(lambda x : x.lower())\n",
    "df['w2'] = df['w2'].apply(lambda x : x.lower())\n",
    "\n",
    "df['sentences1'] = df[['w1','w2']].progress_apply(lambda x : get_wiki_sentences(x['w1'],x['w2']), axis=1)\n",
    "df['sentences2'] = df[['w1','w2']].progress_apply(lambda x : get_wiki_sentences(x['w2'],x['w1']), axis=1)\n",
    "\n",
    "wiki_sentences += df.sentences1.sum()\n",
    "wiki_sentences += df.sentences2.sum()\n",
    "print(\"Scraped sentences :\" + str(len(wiki_sentences)))\n",
    "\n",
    "### Saving Results\n",
    "with open(OUTPUT_DIR + 'wiki_sentences_large.pickle', 'wb') as file:\n",
    "    pickle.dump(wiki_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5bc7e",
   "metadata": {},
   "source": [
    "## <font color='dark-orange'>Gutenberg Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb8e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "austen-persuasion.txt\n",
      "austen-sense.txt\n",
      "bible-kjv.txt\n",
      "blake-poems.txt\n",
      "bryant-stories.txt\n",
      "burgess-busterbrown.txt\n",
      "carroll-alice.txt\n",
      "chesterton-ball.txt\n",
      "chesterton-brown.txt\n",
      "chesterton-thursday.txt\n",
      "edgeworth-parents.txt\n",
      "melville-moby_dick.txt\n",
      "milton-paradise.txt\n",
      "shakespeare-caesar.txt\n",
      "shakespeare-hamlet.txt\n",
      "shakespeare-macbeth.txt\n",
      "whitman-leaves.txt\n",
      "Scraped sentences :98552\n"
     ]
    }
   ],
   "source": [
    "gutenberg_sentences = []\n",
    "\n",
    "for fileid in gutenberg.fileids():   \n",
    "    print(fileid)\n",
    "    for sent_words in gutenberg.sents(fileid):\n",
    "        sentence = ' '.join(sent_words)\n",
    "        gutenberg_sentences.append(sentence)\n",
    "\n",
    "print(\"Scraped sentences :\" + str(len(gutenberg_sentences)))        \n",
    "\n",
    "### Saving Results\n",
    "with open(OUTPUT_DIR + 'gutenberg_sentences_large.pickle', 'wb') as file:\n",
    "    pickle.dump(gutenberg_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d0021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
